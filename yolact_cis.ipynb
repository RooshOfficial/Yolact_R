{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkQP1aNjXIIJoa6i11cgmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RooshOfficial/Yolact_R/blob/main/yolact_cis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z8PMOeAOeYfu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "backbone_selected_layers=[1, 2, 3]\n",
        "\n",
        "class GhostModule(nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dw_size=3, ratio=2, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(GhostModule, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.weight = None\n",
        "        self.ratio = ratio\n",
        "        self.dw_size = dw_size\n",
        "        self.dw_dilation = (dw_size - 1) // 2\n",
        "        self.init_channels = math.ceil(out_channels / ratio)\n",
        "        self.new_channels = self.init_channels * (ratio - 1)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(self.in_channels, self.init_channels, kernel_size, self.stride, padding=self.padding)\n",
        "        self.conv2 = nn.Conv2d(self.init_channels, self.new_channels, self.dw_size, 1, padding=int(self.dw_size/2), groups=self.init_channels)\n",
        "\n",
        "\n",
        "        self.weight1 = nn.Parameter(torch.Tensor(self.init_channels, self.in_channels, kernel_size, kernel_size))\n",
        "        self.bn1 = nn.BatchNorm2d(self.init_channels)\n",
        "        if self.new_channels > 0:\n",
        "            self.weight2 = nn.Parameter(torch.Tensor(self.new_channels, 1, self.dw_size, self.dw_size))\n",
        "            self.bn2 = nn.BatchNorm2d(self.out_channels - self.init_channels)\n",
        "\n",
        "        if bias:\n",
        "            self.bias =nn.Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_custome_parameters()\n",
        "\n",
        "    def reset_custome_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight1, a=math.sqrt(5))\n",
        "        if self.new_channels > 0:\n",
        "            nn.init.kaiming_uniform_(self.weight2, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x1 = self.conv1(input)\n",
        "        if self.new_channels == 0:\n",
        "            return x1\n",
        "        x2 = self.conv2(x1)\n",
        "        x2 = x2[:, :self.out_channels - self.init_channels, :, :]\n",
        "        x = torch.cat([x1, x2], 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, s=4, d=3):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return GhostModule(in_planes, out_planes, kernel_size=3, dw_size=d, ratio=s,\n",
        "                     stride=stride, padding=1, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GhostBottleneck(nn.Module):\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "        self.conv1 = GhostModule(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = GhostModule(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.conv3 = GhostModule(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "aIyZDN0VngPQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GhostResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, block= GhostBottleneck, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_base_layers = len(layers)\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.channels = []\n",
        "        self.norm_layer = norm_layer\n",
        "        self.inplanes = 64\n",
        "        # (3, 550, 550) to (64, 138, 138) stage\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.make_layer(block, 64, layers[0])              # stage 2\n",
        "        self.make_layer(block, 128, layers[1], stride=2)   # stage 3\n",
        "        self.make_layer(block, 256, layers[2], stride=2)   # stage 4\n",
        "        self.make_layer(block, 512, layers[3], stride=2)   # stage 5\n",
        "\n",
        "\n",
        "    def make_layer(self, block, planes, blocks, stride=1):\n",
        "\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                                 kernel_size=1, stride=stride, bias=False),\n",
        "                                       self.norm_layer(planes * block.expansion))\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample, self.norm_layer)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, norm_layer=self.norm_layer))\n",
        "\n",
        "        layer = nn.Sequential(*layers)\n",
        "\n",
        "        self.channels.append(planes * block.expansion)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Returns a list of convouts for each layer. \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        outs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            outs.append(x)\n",
        "\n",
        "        return tuple(outs)\n",
        "\n",
        "\n",
        "    def init_backbone(self, path):\n",
        "        \"\"\" Initializes the backbone weights for training. \"\"\"\n",
        "        state_dict = torch.load(path)\n",
        "\n",
        "        keys = list(state_dict)\n",
        "        for key in keys:\n",
        "            if key.startswith('layer'):\n",
        "                idx = int(key[5])\n",
        "                new_key = 'layers.' + str(idx - 1) + key[6:]\n",
        "                state_dict[new_key] = state_dict.pop(key)\n",
        "\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def add_layer(self, conv_channels=1024, downsample=2, depth=1, block=GhostBottleneck):\n",
        "        \"\"\" Add a downsample layer to the backbone as per what SSD does. \"\"\"\n",
        "        self.make_layer(block, conv_channels // block.expansion, blocks=depth, stride=downsample)\n",
        "\n",
        "\n",
        "\n",
        "def construct_backbone(cfg_backbone = GhostResNet ):\n",
        "    # resnet101 has 3, 4, 23, 3 blocks for each stage\n",
        "    backbone = cfg_backbone([3, 4, 23, 3])\n",
        "\n",
        "    selected_layers=[1, 2, 3]\n",
        "    num_layers = max(selected_layers) + 1\n",
        "\n",
        "    while len(backbone.layers) < num_layers:\n",
        "        backbone.add_layer()\n",
        "\n",
        "    return backbone\n",
        "\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = construct_backbone().to(device)\n",
        "summary(model, (3, 550, 550))\n",
        "input=torch.randn(1, 3, 550, 550)\n",
        "backbone=construct_backbone()(input)\n",
        "\n",
        "print('ghost-resnet output feature :', len(backbone))\n",
        "print('C2 output shape : ', backbone[0].shape)\n",
        "print('C3 output shape : ', backbone[1].shape)\n",
        "print('C4 output shape : ', backbone[2].shape)\n",
        "print('C5 output shape : ', backbone[3].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phJozlyUoaGw",
        "outputId": "2b845b4b-e245-4aa1-b78a-bb431286b1ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 275, 275]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 275, 275]             128\n",
            "              ReLU-3         [-1, 64, 275, 275]               0\n",
            "         MaxPool2d-4         [-1, 64, 138, 138]               0\n",
            "            Conv2d-5         [-1, 32, 138, 138]           2,080\n",
            "            Conv2d-6         [-1, 32, 138, 138]             320\n",
            "       GhostModule-7         [-1, 64, 138, 138]               0\n",
            "       BatchNorm2d-8         [-1, 64, 138, 138]             128\n",
            "              ReLU-9         [-1, 64, 138, 138]               0\n",
            "           Conv2d-10         [-1, 32, 138, 138]          18,464\n",
            "           Conv2d-11         [-1, 32, 138, 138]             320\n",
            "      GhostModule-12         [-1, 64, 138, 138]               0\n",
            "      BatchNorm2d-13         [-1, 64, 138, 138]             128\n",
            "             ReLU-14         [-1, 64, 138, 138]               0\n",
            "           Conv2d-15        [-1, 128, 138, 138]           8,320\n",
            "           Conv2d-16        [-1, 128, 138, 138]           1,280\n",
            "      GhostModule-17        [-1, 256, 138, 138]               0\n",
            "      BatchNorm2d-18        [-1, 256, 138, 138]             512\n",
            "           Conv2d-19        [-1, 256, 138, 138]          16,384\n",
            "      BatchNorm2d-20        [-1, 256, 138, 138]             512\n",
            "             ReLU-21        [-1, 256, 138, 138]               0\n",
            "  GhostBottleneck-22        [-1, 256, 138, 138]               0\n",
            "           Conv2d-23         [-1, 32, 138, 138]           8,224\n",
            "           Conv2d-24         [-1, 32, 138, 138]             320\n",
            "      GhostModule-25         [-1, 64, 138, 138]               0\n",
            "      BatchNorm2d-26         [-1, 64, 138, 138]             128\n",
            "             ReLU-27         [-1, 64, 138, 138]               0\n",
            "           Conv2d-28         [-1, 32, 138, 138]          18,464\n",
            "           Conv2d-29         [-1, 32, 138, 138]             320\n",
            "      GhostModule-30         [-1, 64, 138, 138]               0\n",
            "      BatchNorm2d-31         [-1, 64, 138, 138]             128\n",
            "             ReLU-32         [-1, 64, 138, 138]               0\n",
            "           Conv2d-33        [-1, 128, 138, 138]           8,320\n",
            "           Conv2d-34        [-1, 128, 138, 138]           1,280\n",
            "      GhostModule-35        [-1, 256, 138, 138]               0\n",
            "      BatchNorm2d-36        [-1, 256, 138, 138]             512\n",
            "             ReLU-37        [-1, 256, 138, 138]               0\n",
            "  GhostBottleneck-38        [-1, 256, 138, 138]               0\n",
            "           Conv2d-39         [-1, 32, 138, 138]           8,224\n",
            "           Conv2d-40         [-1, 32, 138, 138]             320\n",
            "      GhostModule-41         [-1, 64, 138, 138]               0\n",
            "      BatchNorm2d-42         [-1, 64, 138, 138]             128\n",
            "             ReLU-43         [-1, 64, 138, 138]               0\n",
            "           Conv2d-44         [-1, 32, 138, 138]          18,464\n",
            "           Conv2d-45         [-1, 32, 138, 138]             320\n",
            "      GhostModule-46         [-1, 64, 138, 138]               0\n",
            "      BatchNorm2d-47         [-1, 64, 138, 138]             128\n",
            "             ReLU-48         [-1, 64, 138, 138]               0\n",
            "           Conv2d-49        [-1, 128, 138, 138]           8,320\n",
            "           Conv2d-50        [-1, 128, 138, 138]           1,280\n",
            "      GhostModule-51        [-1, 256, 138, 138]               0\n",
            "      BatchNorm2d-52        [-1, 256, 138, 138]             512\n",
            "             ReLU-53        [-1, 256, 138, 138]               0\n",
            "  GhostBottleneck-54        [-1, 256, 138, 138]               0\n",
            "           Conv2d-55         [-1, 64, 138, 138]          16,448\n",
            "           Conv2d-56         [-1, 64, 138, 138]             640\n",
            "      GhostModule-57        [-1, 128, 138, 138]               0\n",
            "      BatchNorm2d-58        [-1, 128, 138, 138]             256\n",
            "             ReLU-59        [-1, 128, 138, 138]               0\n",
            "           Conv2d-60           [-1, 64, 69, 69]          73,792\n",
            "           Conv2d-61           [-1, 64, 69, 69]             640\n",
            "      GhostModule-62          [-1, 128, 69, 69]               0\n",
            "      BatchNorm2d-63          [-1, 128, 69, 69]             256\n",
            "             ReLU-64          [-1, 128, 69, 69]               0\n",
            "           Conv2d-65          [-1, 256, 69, 69]          33,024\n",
            "           Conv2d-66          [-1, 256, 69, 69]           2,560\n",
            "      GhostModule-67          [-1, 512, 69, 69]               0\n",
            "      BatchNorm2d-68          [-1, 512, 69, 69]           1,024\n",
            "           Conv2d-69          [-1, 512, 69, 69]         131,072\n",
            "      BatchNorm2d-70          [-1, 512, 69, 69]           1,024\n",
            "             ReLU-71          [-1, 512, 69, 69]               0\n",
            "  GhostBottleneck-72          [-1, 512, 69, 69]               0\n",
            "           Conv2d-73           [-1, 64, 69, 69]          32,832\n",
            "           Conv2d-74           [-1, 64, 69, 69]             640\n",
            "      GhostModule-75          [-1, 128, 69, 69]               0\n",
            "      BatchNorm2d-76          [-1, 128, 69, 69]             256\n",
            "             ReLU-77          [-1, 128, 69, 69]               0\n",
            "           Conv2d-78           [-1, 64, 69, 69]          73,792\n",
            "           Conv2d-79           [-1, 64, 69, 69]             640\n",
            "      GhostModule-80          [-1, 128, 69, 69]               0\n",
            "      BatchNorm2d-81          [-1, 128, 69, 69]             256\n",
            "             ReLU-82          [-1, 128, 69, 69]               0\n",
            "           Conv2d-83          [-1, 256, 69, 69]          33,024\n",
            "           Conv2d-84          [-1, 256, 69, 69]           2,560\n",
            "      GhostModule-85          [-1, 512, 69, 69]               0\n",
            "      BatchNorm2d-86          [-1, 512, 69, 69]           1,024\n",
            "             ReLU-87          [-1, 512, 69, 69]               0\n",
            "  GhostBottleneck-88          [-1, 512, 69, 69]               0\n",
            "           Conv2d-89           [-1, 64, 69, 69]          32,832\n",
            "           Conv2d-90           [-1, 64, 69, 69]             640\n",
            "      GhostModule-91          [-1, 128, 69, 69]               0\n",
            "      BatchNorm2d-92          [-1, 128, 69, 69]             256\n",
            "             ReLU-93          [-1, 128, 69, 69]               0\n",
            "           Conv2d-94           [-1, 64, 69, 69]          73,792\n",
            "           Conv2d-95           [-1, 64, 69, 69]             640\n",
            "      GhostModule-96          [-1, 128, 69, 69]               0\n",
            "      BatchNorm2d-97          [-1, 128, 69, 69]             256\n",
            "             ReLU-98          [-1, 128, 69, 69]               0\n",
            "           Conv2d-99          [-1, 256, 69, 69]          33,024\n",
            "          Conv2d-100          [-1, 256, 69, 69]           2,560\n",
            "     GhostModule-101          [-1, 512, 69, 69]               0\n",
            "     BatchNorm2d-102          [-1, 512, 69, 69]           1,024\n",
            "            ReLU-103          [-1, 512, 69, 69]               0\n",
            " GhostBottleneck-104          [-1, 512, 69, 69]               0\n",
            "          Conv2d-105           [-1, 64, 69, 69]          32,832\n",
            "          Conv2d-106           [-1, 64, 69, 69]             640\n",
            "     GhostModule-107          [-1, 128, 69, 69]               0\n",
            "     BatchNorm2d-108          [-1, 128, 69, 69]             256\n",
            "            ReLU-109          [-1, 128, 69, 69]               0\n",
            "          Conv2d-110           [-1, 64, 69, 69]          73,792\n",
            "          Conv2d-111           [-1, 64, 69, 69]             640\n",
            "     GhostModule-112          [-1, 128, 69, 69]               0\n",
            "     BatchNorm2d-113          [-1, 128, 69, 69]             256\n",
            "            ReLU-114          [-1, 128, 69, 69]               0\n",
            "          Conv2d-115          [-1, 256, 69, 69]          33,024\n",
            "          Conv2d-116          [-1, 256, 69, 69]           2,560\n",
            "     GhostModule-117          [-1, 512, 69, 69]               0\n",
            "     BatchNorm2d-118          [-1, 512, 69, 69]           1,024\n",
            "            ReLU-119          [-1, 512, 69, 69]               0\n",
            " GhostBottleneck-120          [-1, 512, 69, 69]               0\n",
            "          Conv2d-121          [-1, 128, 69, 69]          65,664\n",
            "          Conv2d-122          [-1, 128, 69, 69]           1,280\n",
            "     GhostModule-123          [-1, 256, 69, 69]               0\n",
            "     BatchNorm2d-124          [-1, 256, 69, 69]             512\n",
            "            ReLU-125          [-1, 256, 69, 69]               0\n",
            "          Conv2d-126          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-127          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-128          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-129          [-1, 256, 35, 35]             512\n",
            "            ReLU-130          [-1, 256, 35, 35]               0\n",
            "          Conv2d-131          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-132          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-133         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-134         [-1, 1024, 35, 35]           2,048\n",
            "          Conv2d-135         [-1, 1024, 35, 35]         524,288\n",
            "     BatchNorm2d-136         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-137         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-138         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-139          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-140          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-141          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-142          [-1, 256, 35, 35]             512\n",
            "            ReLU-143          [-1, 256, 35, 35]               0\n",
            "          Conv2d-144          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-145          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-146          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-147          [-1, 256, 35, 35]             512\n",
            "            ReLU-148          [-1, 256, 35, 35]               0\n",
            "          Conv2d-149          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-150          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-151         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-152         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-153         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-154         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-155          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-156          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-157          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-158          [-1, 256, 35, 35]             512\n",
            "            ReLU-159          [-1, 256, 35, 35]               0\n",
            "          Conv2d-160          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-161          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-162          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-163          [-1, 256, 35, 35]             512\n",
            "            ReLU-164          [-1, 256, 35, 35]               0\n",
            "          Conv2d-165          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-166          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-167         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-168         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-169         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-170         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-171          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-172          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-173          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-174          [-1, 256, 35, 35]             512\n",
            "            ReLU-175          [-1, 256, 35, 35]               0\n",
            "          Conv2d-176          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-177          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-178          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-179          [-1, 256, 35, 35]             512\n",
            "            ReLU-180          [-1, 256, 35, 35]               0\n",
            "          Conv2d-181          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-182          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-183         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-184         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-185         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-186         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-187          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-188          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-189          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-190          [-1, 256, 35, 35]             512\n",
            "            ReLU-191          [-1, 256, 35, 35]               0\n",
            "          Conv2d-192          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-193          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-194          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-195          [-1, 256, 35, 35]             512\n",
            "            ReLU-196          [-1, 256, 35, 35]               0\n",
            "          Conv2d-197          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-198          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-199         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-200         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-201         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-202         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-203          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-204          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-205          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-206          [-1, 256, 35, 35]             512\n",
            "            ReLU-207          [-1, 256, 35, 35]               0\n",
            "          Conv2d-208          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-209          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-210          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-211          [-1, 256, 35, 35]             512\n",
            "            ReLU-212          [-1, 256, 35, 35]               0\n",
            "          Conv2d-213          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-214          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-215         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-216         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-217         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-218         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-219          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-220          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-221          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-222          [-1, 256, 35, 35]             512\n",
            "            ReLU-223          [-1, 256, 35, 35]               0\n",
            "          Conv2d-224          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-225          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-226          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-227          [-1, 256, 35, 35]             512\n",
            "            ReLU-228          [-1, 256, 35, 35]               0\n",
            "          Conv2d-229          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-230          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-231         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-232         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-233         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-234         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-235          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-236          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-237          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-238          [-1, 256, 35, 35]             512\n",
            "            ReLU-239          [-1, 256, 35, 35]               0\n",
            "          Conv2d-240          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-241          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-242          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-243          [-1, 256, 35, 35]             512\n",
            "            ReLU-244          [-1, 256, 35, 35]               0\n",
            "          Conv2d-245          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-246          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-247         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-248         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-249         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-250         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-251          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-252          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-253          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-254          [-1, 256, 35, 35]             512\n",
            "            ReLU-255          [-1, 256, 35, 35]               0\n",
            "          Conv2d-256          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-257          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-258          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-259          [-1, 256, 35, 35]             512\n",
            "            ReLU-260          [-1, 256, 35, 35]               0\n",
            "          Conv2d-261          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-262          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-263         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-264         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-265         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-266         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-267          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-268          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-269          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-270          [-1, 256, 35, 35]             512\n",
            "            ReLU-271          [-1, 256, 35, 35]               0\n",
            "          Conv2d-272          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-273          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-274          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-275          [-1, 256, 35, 35]             512\n",
            "            ReLU-276          [-1, 256, 35, 35]               0\n",
            "          Conv2d-277          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-278          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-279         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-280         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-281         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-282         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-283          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-284          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-285          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-286          [-1, 256, 35, 35]             512\n",
            "            ReLU-287          [-1, 256, 35, 35]               0\n",
            "          Conv2d-288          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-289          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-290          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-291          [-1, 256, 35, 35]             512\n",
            "            ReLU-292          [-1, 256, 35, 35]               0\n",
            "          Conv2d-293          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-294          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-295         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-296         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-297         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-298         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-299          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-300          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-301          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-302          [-1, 256, 35, 35]             512\n",
            "            ReLU-303          [-1, 256, 35, 35]               0\n",
            "          Conv2d-304          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-305          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-306          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-307          [-1, 256, 35, 35]             512\n",
            "            ReLU-308          [-1, 256, 35, 35]               0\n",
            "          Conv2d-309          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-310          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-311         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-312         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-313         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-314         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-315          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-316          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-317          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-318          [-1, 256, 35, 35]             512\n",
            "            ReLU-319          [-1, 256, 35, 35]               0\n",
            "          Conv2d-320          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-321          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-322          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-323          [-1, 256, 35, 35]             512\n",
            "            ReLU-324          [-1, 256, 35, 35]               0\n",
            "          Conv2d-325          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-326          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-327         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-328         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-329         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-330         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-331          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-332          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-333          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-334          [-1, 256, 35, 35]             512\n",
            "            ReLU-335          [-1, 256, 35, 35]               0\n",
            "          Conv2d-336          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-337          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-338          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-339          [-1, 256, 35, 35]             512\n",
            "            ReLU-340          [-1, 256, 35, 35]               0\n",
            "          Conv2d-341          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-342          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-343         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-344         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-345         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-346         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-347          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-348          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-349          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-350          [-1, 256, 35, 35]             512\n",
            "            ReLU-351          [-1, 256, 35, 35]               0\n",
            "          Conv2d-352          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-353          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-354          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-355          [-1, 256, 35, 35]             512\n",
            "            ReLU-356          [-1, 256, 35, 35]               0\n",
            "          Conv2d-357          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-358          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-359         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-360         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-361         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-362         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-363          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-364          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-365          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-366          [-1, 256, 35, 35]             512\n",
            "            ReLU-367          [-1, 256, 35, 35]               0\n",
            "          Conv2d-368          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-369          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-370          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-371          [-1, 256, 35, 35]             512\n",
            "            ReLU-372          [-1, 256, 35, 35]               0\n",
            "          Conv2d-373          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-374          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-375         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-376         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-377         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-378         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-379          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-380          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-381          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-382          [-1, 256, 35, 35]             512\n",
            "            ReLU-383          [-1, 256, 35, 35]               0\n",
            "          Conv2d-384          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-385          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-386          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-387          [-1, 256, 35, 35]             512\n",
            "            ReLU-388          [-1, 256, 35, 35]               0\n",
            "          Conv2d-389          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-390          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-391         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-392         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-393         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-394         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-395          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-396          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-397          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-398          [-1, 256, 35, 35]             512\n",
            "            ReLU-399          [-1, 256, 35, 35]               0\n",
            "          Conv2d-400          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-401          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-402          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-403          [-1, 256, 35, 35]             512\n",
            "            ReLU-404          [-1, 256, 35, 35]               0\n",
            "          Conv2d-405          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-406          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-407         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-408         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-409         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-410         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-411          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-412          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-413          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-414          [-1, 256, 35, 35]             512\n",
            "            ReLU-415          [-1, 256, 35, 35]               0\n",
            "          Conv2d-416          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-417          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-418          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-419          [-1, 256, 35, 35]             512\n",
            "            ReLU-420          [-1, 256, 35, 35]               0\n",
            "          Conv2d-421          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-422          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-423         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-424         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-425         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-426         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-427          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-428          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-429          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-430          [-1, 256, 35, 35]             512\n",
            "            ReLU-431          [-1, 256, 35, 35]               0\n",
            "          Conv2d-432          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-433          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-434          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-435          [-1, 256, 35, 35]             512\n",
            "            ReLU-436          [-1, 256, 35, 35]               0\n",
            "          Conv2d-437          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-438          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-439         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-440         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-441         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-442         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-443          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-444          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-445          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-446          [-1, 256, 35, 35]             512\n",
            "            ReLU-447          [-1, 256, 35, 35]               0\n",
            "          Conv2d-448          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-449          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-450          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-451          [-1, 256, 35, 35]             512\n",
            "            ReLU-452          [-1, 256, 35, 35]               0\n",
            "          Conv2d-453          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-454          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-455         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-456         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-457         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-458         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-459          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-460          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-461          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-462          [-1, 256, 35, 35]             512\n",
            "            ReLU-463          [-1, 256, 35, 35]               0\n",
            "          Conv2d-464          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-465          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-466          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-467          [-1, 256, 35, 35]             512\n",
            "            ReLU-468          [-1, 256, 35, 35]               0\n",
            "          Conv2d-469          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-470          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-471         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-472         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-473         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-474         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-475          [-1, 128, 35, 35]         131,200\n",
            "          Conv2d-476          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-477          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-478          [-1, 256, 35, 35]             512\n",
            "            ReLU-479          [-1, 256, 35, 35]               0\n",
            "          Conv2d-480          [-1, 128, 35, 35]         295,040\n",
            "          Conv2d-481          [-1, 128, 35, 35]           1,280\n",
            "     GhostModule-482          [-1, 256, 35, 35]               0\n",
            "     BatchNorm2d-483          [-1, 256, 35, 35]             512\n",
            "            ReLU-484          [-1, 256, 35, 35]               0\n",
            "          Conv2d-485          [-1, 512, 35, 35]         131,584\n",
            "          Conv2d-486          [-1, 512, 35, 35]           5,120\n",
            "     GhostModule-487         [-1, 1024, 35, 35]               0\n",
            "     BatchNorm2d-488         [-1, 1024, 35, 35]           2,048\n",
            "            ReLU-489         [-1, 1024, 35, 35]               0\n",
            " GhostBottleneck-490         [-1, 1024, 35, 35]               0\n",
            "          Conv2d-491          [-1, 256, 35, 35]         262,400\n",
            "          Conv2d-492          [-1, 256, 35, 35]           2,560\n",
            "     GhostModule-493          [-1, 512, 35, 35]               0\n",
            "     BatchNorm2d-494          [-1, 512, 35, 35]           1,024\n",
            "            ReLU-495          [-1, 512, 35, 35]               0\n",
            "          Conv2d-496          [-1, 256, 18, 18]       1,179,904\n",
            "          Conv2d-497          [-1, 256, 18, 18]           2,560\n",
            "     GhostModule-498          [-1, 512, 18, 18]               0\n",
            "     BatchNorm2d-499          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-500          [-1, 512, 18, 18]               0\n",
            "          Conv2d-501         [-1, 1024, 18, 18]         525,312\n",
            "          Conv2d-502         [-1, 1024, 18, 18]          10,240\n",
            "     GhostModule-503         [-1, 2048, 18, 18]               0\n",
            "     BatchNorm2d-504         [-1, 2048, 18, 18]           4,096\n",
            "          Conv2d-505         [-1, 2048, 18, 18]       2,097,152\n",
            "     BatchNorm2d-506         [-1, 2048, 18, 18]           4,096\n",
            "            ReLU-507         [-1, 2048, 18, 18]               0\n",
            " GhostBottleneck-508         [-1, 2048, 18, 18]               0\n",
            "          Conv2d-509          [-1, 256, 18, 18]         524,544\n",
            "          Conv2d-510          [-1, 256, 18, 18]           2,560\n",
            "     GhostModule-511          [-1, 512, 18, 18]               0\n",
            "     BatchNorm2d-512          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-513          [-1, 512, 18, 18]               0\n",
            "          Conv2d-514          [-1, 256, 18, 18]       1,179,904\n",
            "          Conv2d-515          [-1, 256, 18, 18]           2,560\n",
            "     GhostModule-516          [-1, 512, 18, 18]               0\n",
            "     BatchNorm2d-517          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-518          [-1, 512, 18, 18]               0\n",
            "          Conv2d-519         [-1, 1024, 18, 18]         525,312\n",
            "          Conv2d-520         [-1, 1024, 18, 18]          10,240\n",
            "     GhostModule-521         [-1, 2048, 18, 18]               0\n",
            "     BatchNorm2d-522         [-1, 2048, 18, 18]           4,096\n",
            "            ReLU-523         [-1, 2048, 18, 18]               0\n",
            " GhostBottleneck-524         [-1, 2048, 18, 18]               0\n",
            "          Conv2d-525          [-1, 256, 18, 18]         524,544\n",
            "          Conv2d-526          [-1, 256, 18, 18]           2,560\n",
            "     GhostModule-527          [-1, 512, 18, 18]               0\n",
            "     BatchNorm2d-528          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-529          [-1, 512, 18, 18]               0\n",
            "          Conv2d-530          [-1, 256, 18, 18]       1,179,904\n",
            "          Conv2d-531          [-1, 256, 18, 18]           2,560\n",
            "     GhostModule-532          [-1, 512, 18, 18]               0\n",
            "     BatchNorm2d-533          [-1, 512, 18, 18]           1,024\n",
            "            ReLU-534          [-1, 512, 18, 18]               0\n",
            "          Conv2d-535         [-1, 1024, 18, 18]         525,312\n",
            "          Conv2d-536         [-1, 1024, 18, 18]          10,240\n",
            "     GhostModule-537         [-1, 2048, 18, 18]               0\n",
            "     BatchNorm2d-538         [-1, 2048, 18, 18]           4,096\n",
            "            ReLU-539         [-1, 2048, 18, 18]               0\n",
            " GhostBottleneck-540         [-1, 2048, 18, 18]               0\n",
            "================================================================\n",
            "Total params: 22,960,128\n",
            "Trainable params: 22,960,128\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 3.46\n",
            "Forward/backward pass size (MB): 3307.39\n",
            "Params size (MB): 87.59\n",
            "Estimated Total Size (MB): 3398.44\n",
            "----------------------------------------------------------------\n",
            "ghost-resnet output feature : 4\n",
            "C2 output shape :  torch.Size([1, 256, 138, 138])\n",
            "C3 output shape :  torch.Size([1, 512, 69, 69])\n",
            "C4 output shape :  torch.Size([1, 1024, 35, 35])\n",
            "C5 output shape :  torch.Size([1, 2048, 18, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1x1 convolution for P5\n",
        "        self.p5_conv = nn.Conv2d(2048, 256, kernel_size=1)\n",
        "\n",
        "        # 1x1 convolutions for C4 and C3\n",
        "        self.c4_conv = nn.Conv2d(1024, 256, kernel_size=1)\n",
        "        self.c3_conv = nn.Conv2d(512, 256, kernel_size=1)\n",
        "\n",
        "        # 3x3 convolutions for N3, N4, N5\n",
        "        self.n3_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.n4_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.n5_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        # Downsampling layers\n",
        "        self.m3_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.m4_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.n6_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.n7_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, c3, c4, c5):\n",
        "        # Calculate P5\n",
        "        p5 = self.p5_conv(c5)\n",
        "\n",
        "        # Calculate P4\n",
        "        p4_upsample = F.interpolate(p5, size=(35,35), mode='bilinear', align_corners=False)\n",
        "        p4_conv = self.c4_conv(c4)\n",
        "        p4 = p4_upsample + p4_conv\n",
        "\n",
        "        # Calculate P3\n",
        "        p3_upsample = F.interpolate(p4, size=(69,69), mode='bilinear', align_corners=False)\n",
        "        p3_conv = self.c3_conv(c3)\n",
        "        p3 = p3_upsample + p3_conv\n",
        "\n",
        "        # Calculate N3\n",
        "        n3 = self.n3_conv(p3)\n",
        "\n",
        "        # Calculate M3 and N4\n",
        "        m3 = self.m3_down(n3)\n",
        "        n4 = self.n4_conv(p4 + m3)\n",
        "\n",
        "        # Calculate M4 and N5\n",
        "        m4 = self.m4_down(n4)\n",
        "        n5 = self.n5_conv(p5 + m4)\n",
        "\n",
        "        # Calculate N6 and N7\n",
        "        n6 = self.n6_down(n5)\n",
        "        n7 = self.n7_down(n6)\n",
        "\n",
        "        return n3, n4, n5, n6, n7\n",
        "\n",
        "c3 = backbone[1]\n",
        "c4 = backbone[2]\n",
        "c5 = backbone[3]\n",
        "\n",
        "dffn = DFFN()\n",
        "n3, n4, n5, n6, n7 = dffn(c3, c4, c5)\n",
        "dffn_outs = n3, n4, n5, n6, n7\n",
        "\n",
        "print(\"N3 size:\", n3.shape)\n",
        "print(\"N4 size:\", n4.shape)\n",
        "print(\"N5 size:\", n5.shape)\n",
        "print(\"N6 size:\", n6.shape)\n",
        "print(\"N7 size:\", n7.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSPnYQjs5wU-",
        "outputId": "13f16d2d-c9ad-44d2-cbab-5f2d586bf921"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N3 size: torch.Size([1, 256, 69, 69])\n",
            "N4 size: torch.Size([1, 256, 35, 35])\n",
            "N5 size: torch.Size([1, 256, 18, 18])\n",
            "N6 size: torch.Size([1, 256, 9, 9])\n",
            "N7 size: torch.Size([1, 256, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N3 the feature map with the highest resolution\n",
        "\n",
        "mask_proto_net = [(256, 3, {'padding': 1}), (256, 3, {'padding': 1}), (256, 3, {'padding': 1}),\n",
        "                  (None, -2, {}), (256, 3, {'padding': 1}), (32, 1, {})]\n",
        "\n",
        "class Protonet(nn.Module) :\n",
        "    def __init__(self, mask_proto_net) :\n",
        "        super().__init__()\n",
        "\n",
        "        self.inplanes=256\n",
        "        self.mask_proto_net = mask_proto_net\n",
        "        self.conv1 = nn.Conv2d(self.inplanes, mask_proto_net[0][0], kernel_size=mask_proto_net[0][1], **mask_proto_net[0][2])\n",
        "        self.conv2 = nn.Conv2d(self.inplanes, mask_proto_net[1][0], kernel_size=mask_proto_net[1][1], **mask_proto_net[1][2])\n",
        "        self.conv3 = nn.Conv2d(self.inplanes, mask_proto_net[2][0], kernel_size=mask_proto_net[2][1], **mask_proto_net[2][2])\n",
        "        self.conv4 = nn.Conv2d(self.inplanes, mask_proto_net[4][0], kernel_size=mask_proto_net[4][1], **mask_proto_net[4][2])\n",
        "        self.conv5 = nn.Conv2d(self.inplanes, mask_proto_net[5][0], kernel_size=mask_proto_net[5][1], **mask_proto_net[5][2])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "        out = F.interpolate(out, scale_factor = -self.mask_proto_net[3][1], mode='bilinear', align_corners=False, **self.mask_proto_net[3][2])\n",
        "        out = self.relu(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv5(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "proto_out=Protonet(mask_proto_net)(n3)\n",
        "print(proto_out)\n",
        "print('-'*50)\n",
        "print('Proto net output shape : ', proto_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRoZCf-8UehD",
        "outputId": "1fc42f2b-801f-429f-a1d5-797c414cdee0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-3.6992e-02, -3.1762e-02, -3.1715e-02,  ..., -4.9166e-02,\n",
            "           -4.6628e-02, -3.9687e-02],\n",
            "          [-4.1543e-02, -3.9857e-02, -4.0299e-02,  ..., -4.9766e-02,\n",
            "           -4.4441e-02, -3.5860e-02],\n",
            "          [-4.0858e-02, -4.2961e-02, -4.3873e-02,  ..., -5.5714e-02,\n",
            "           -5.0131e-02, -3.8075e-02],\n",
            "          ...,\n",
            "          [-4.6255e-02, -5.0444e-02, -5.0349e-02,  ..., -4.8980e-02,\n",
            "           -5.2253e-02, -4.5944e-02],\n",
            "          [-4.5388e-02, -4.6656e-02, -4.6662e-02,  ..., -4.6578e-02,\n",
            "           -5.0253e-02, -4.6804e-02],\n",
            "          [-3.6967e-02, -3.6889e-02, -3.5172e-02,  ..., -4.1175e-02,\n",
            "           -4.3050e-02, -4.6559e-02]],\n",
            "\n",
            "         [[-2.1900e-02, -2.2436e-02, -2.5400e-02,  ..., -2.9217e-02,\n",
            "           -2.6009e-02, -2.5228e-02],\n",
            "          [-2.1987e-02, -2.8460e-02, -3.4016e-02,  ..., -3.7938e-02,\n",
            "           -3.3104e-02, -2.6514e-02],\n",
            "          [-2.6655e-02, -3.1347e-02, -3.4465e-02,  ..., -3.9826e-02,\n",
            "           -3.6048e-02, -2.6599e-02],\n",
            "          ...,\n",
            "          [-3.2025e-02, -3.5741e-02, -3.7541e-02,  ..., -3.2651e-02,\n",
            "           -2.8871e-02, -2.1522e-02],\n",
            "          [-2.9770e-02, -3.5683e-02, -3.6801e-02,  ..., -3.0420e-02,\n",
            "           -2.9047e-02, -2.4383e-02],\n",
            "          [-2.6961e-02, -3.1080e-02, -2.9891e-02,  ..., -2.2608e-02,\n",
            "           -2.2196e-02, -2.4338e-02]],\n",
            "\n",
            "         [[ 8.8498e-03,  7.4390e-03,  7.9466e-03,  ...,  7.9299e-03,\n",
            "            9.6169e-03,  1.2871e-02],\n",
            "          [ 2.9860e-03, -1.4109e-03, -3.6130e-03,  ..., -1.1488e-03,\n",
            "            3.6477e-05,  6.1991e-03],\n",
            "          [ 2.4867e-03, -3.4396e-03, -6.0508e-03,  ..., -2.2578e-03,\n",
            "           -2.0730e-03,  6.9702e-03],\n",
            "          ...,\n",
            "          [ 1.3904e-02, -1.4378e-03, -7.9240e-03,  ..., -2.0659e-02,\n",
            "           -1.6306e-02,  1.9529e-03],\n",
            "          [ 1.2595e-02, -3.9423e-03, -8.9132e-03,  ..., -1.7356e-02,\n",
            "           -1.3225e-02,  4.5761e-03],\n",
            "          [ 8.2797e-03, -6.5936e-03, -8.3100e-03,  ..., -7.7094e-03,\n",
            "           -5.3095e-03,  6.6829e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3427e-02, -3.1380e-02, -2.7936e-02,  ..., -2.2077e-02,\n",
            "           -2.1935e-02, -2.1334e-02],\n",
            "          [-3.6924e-02, -3.7731e-02, -3.2767e-02,  ..., -3.2002e-02,\n",
            "           -3.2564e-02, -2.8327e-02],\n",
            "          [-3.7749e-02, -3.6732e-02, -3.1712e-02,  ..., -3.0258e-02,\n",
            "           -3.2839e-02, -2.9962e-02],\n",
            "          ...,\n",
            "          [-3.0301e-02, -2.3940e-02, -2.5094e-02,  ..., -1.3695e-02,\n",
            "           -2.1736e-02, -2.6185e-02],\n",
            "          [-2.9007e-02, -2.4091e-02, -2.5750e-02,  ..., -1.4872e-02,\n",
            "           -2.1673e-02, -2.7811e-02],\n",
            "          [-3.0657e-02, -2.6150e-02, -2.5700e-02,  ..., -2.3081e-02,\n",
            "           -2.5469e-02, -2.9453e-02]],\n",
            "\n",
            "         [[ 1.9939e-02,  2.9472e-02,  3.1590e-02,  ...,  2.1626e-02,\n",
            "            2.0682e-02,  1.8167e-02],\n",
            "          [ 2.2420e-02,  3.5584e-02,  3.8003e-02,  ...,  2.4383e-02,\n",
            "            2.2027e-02,  1.5842e-02],\n",
            "          [ 1.9989e-02,  3.8322e-02,  4.3570e-02,  ...,  3.1785e-02,\n",
            "            2.7414e-02,  1.4662e-02],\n",
            "          ...,\n",
            "          [ 3.0711e-02,  4.0843e-02,  4.3324e-02,  ...,  3.9844e-02,\n",
            "            3.2181e-02,  1.7689e-02],\n",
            "          [ 2.8544e-02,  3.4892e-02,  3.6425e-02,  ...,  3.5250e-02,\n",
            "            2.8078e-02,  1.6149e-02],\n",
            "          [ 2.8393e-02,  3.4884e-02,  3.8325e-02,  ...,  3.5414e-02,\n",
            "            2.9526e-02,  1.6849e-02]],\n",
            "\n",
            "         [[ 4.9478e-03,  9.2686e-03,  7.7378e-03,  ..., -2.7798e-03,\n",
            "           -4.1043e-03,  4.6148e-04],\n",
            "          [ 1.2062e-02,  1.7046e-02,  1.7864e-02,  ..., -2.1315e-04,\n",
            "           -1.9481e-03,  3.8844e-03],\n",
            "          [ 8.7664e-03,  1.3714e-02,  1.2355e-02,  ..., -6.5564e-04,\n",
            "           -1.7993e-04,  7.3429e-03],\n",
            "          ...,\n",
            "          [-3.1501e-03, -1.0942e-03, -7.2251e-04,  ...,  1.8813e-02,\n",
            "            1.5272e-02,  1.7717e-02],\n",
            "          [-3.9816e-03, -1.3764e-03, -1.4591e-03,  ...,  1.4609e-02,\n",
            "            1.2943e-02,  1.7606e-02],\n",
            "          [-5.0952e-03, -3.0995e-03, -4.5975e-03,  ..., -5.1589e-04,\n",
            "            5.1679e-06,  7.3318e-03]]]], grad_fn=<ConvolutionBackward0>)\n",
            "--------------------------------------------------\n",
            "Proto net output shape :  torch.Size([1, 32, 138, 138])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coef_dim=proto_out.shape[1]\n",
        "num_classes=81\n",
        "aspect_ratios: [1, 1 / 2, 2]\n",
        "class PredictionModule(nn.Module):\n",
        "    def __init__(self, in_channels, coef_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = 81\n",
        "        self.coef_dim = coef_dim\n",
        "        self.num_priors = 3            # num of anchor box for each pixel of feature map\n",
        "\n",
        "        self.upfeature = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        out_channels = 256\n",
        "        self.bbox_layer = nn.Conv2d(out_channels, self.num_priors * 4, kernel_size=3, padding=1)\n",
        "        self.conf_layer = nn.Conv2d(out_channels, self.num_priors * self.num_classes, kernel_size=3, padding=1)\n",
        "        self.mask_layer = nn.Conv2d(out_channels, self.num_priors * self.coef_dim, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upfeature(x)\n",
        "        x = self.relu(x)\n",
        "        conf = self.conf_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n",
        "        bbox = self.bbox_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n",
        "        coef_test = self.mask_layer(x)\n",
        "        print('mask layer output shape : ', coef_test.shape)\n",
        "        coef = self.mask_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.coef_dim)\n",
        "        # mask_layer output shape : [n, 96, 69, 69] / In order to make it's shape [n, 69*69*3, 32], use permute and contiguous.\n",
        "        print('Changed shape : ', coef.shape)\n",
        "        coef = torch.tanh(coef)\n",
        "\n",
        "        return {'box': bbox, 'class': conf, 'coef': coef}\n",
        "prediction_layers = nn.ModuleList()\n",
        "prediction_layers.append(PredictionModule(in_channels=256, coef_dim=coef_dim))\n",
        "print(prediction_layers[0](dffn_outs[0]))\n",
        "\n",
        "predictions = {'box': [], 'class': [], 'coef': []}\n",
        "for i in range(len(dffn_outs)) :\n",
        "    p=prediction_layers[0](dffn_outs[i])\n",
        "    for key, value in p.items() :\n",
        "        predictions[key].append(value)\n",
        "print(predictions.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5LTte2PanMw",
        "outputId": "b56329c9-93ec-4f3c-9046-e5f228473bc4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask layer output shape :  torch.Size([1, 96, 69, 69])\n",
            "Changed shape :  torch.Size([1, 14283, 32])\n",
            "{'box': tensor([[[ 0.0699, -0.1327,  0.0862, -0.0083],\n",
            "         [ 0.1819,  0.2032,  0.1766, -0.0538],\n",
            "         [ 0.1502, -0.2661, -0.3464,  0.3481],\n",
            "         ...,\n",
            "         [ 0.0700,  0.1908,  0.3150, -0.0221],\n",
            "         [-0.0756, -0.1402,  0.3611, -0.1429],\n",
            "         [ 0.2203, -0.0656,  0.0964,  0.2485]]], grad_fn=<ViewBackward0>), 'class': tensor([[[ 0.3049, -0.0413,  0.1596,  ...,  0.2430, -0.0452,  0.0548],\n",
            "         [ 0.1053,  0.0050,  0.1711,  ..., -0.2070, -0.1530, -0.0506],\n",
            "         [ 0.1998,  0.0952,  0.1474,  ...,  0.3210, -0.1838, -0.3433],\n",
            "         ...,\n",
            "         [-0.1159, -0.1293, -0.1754,  ...,  0.0729,  0.1895, -0.2376],\n",
            "         [-0.1461,  0.0587, -0.0773,  ..., -0.0358,  0.0603,  0.2492],\n",
            "         [-0.0545, -0.1177, -0.0736,  ...,  0.0749,  0.1883,  0.0668]]],\n",
            "       grad_fn=<ViewBackward0>), 'coef': tensor([[[-0.0513, -0.1265,  0.0539,  ...,  0.3950, -0.0687, -0.0295],\n",
            "         [-0.0879,  0.0662, -0.0631,  ..., -0.2069,  0.0652,  0.1290],\n",
            "         [ 0.0026,  0.0219, -0.0573,  ...,  0.1392,  0.0346, -0.1323],\n",
            "         ...,\n",
            "         [ 0.0187,  0.0725, -0.1215,  ..., -0.1848, -0.1557, -0.1116],\n",
            "         [-0.4273,  0.0624,  0.1514,  ...,  0.0555,  0.1130, -0.0767],\n",
            "         [ 0.0843,  0.0044, -0.0170,  ...,  0.0508, -0.1293,  0.0416]]],\n",
            "       grad_fn=<TanhBackward0>)}\n",
            "mask layer output shape :  torch.Size([1, 96, 69, 69])\n",
            "Changed shape :  torch.Size([1, 14283, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 35, 35])\n",
            "Changed shape :  torch.Size([1, 3675, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 18, 18])\n",
            "Changed shape :  torch.Size([1, 972, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 9, 9])\n",
            "Changed shape :  torch.Size([1, 243, 32])\n",
            "mask layer output shape :  torch.Size([1, 96, 5, 5])\n",
            "Changed shape :  torch.Size([1, 75, 32])\n",
            "dict_keys(['box', 'class', 'coef'])\n"
          ]
        }
      ]
    }
  ]
}