{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1euGHbkHOuNAzEz_TQC1W_PR0F5GNFUdt",
      "authorship_tag": "ABX9TyPtvCJ6j89a9YQfr0inwTpX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/yolact/results/images/', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/yolact/results/trt_images/', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/yolact/weights/', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/yolact/trt_files/', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/yolact/tensorboard_log/', exist_ok=True)\n",
        "\n",
        "COLORS = np.array([[0, 0, 0], [244, 67, 54], [233, 30, 99], [156, 39, 176], [103, 58, 183], [100, 30, 60],\n",
        "                   [63, 81, 181], [33, 150, 243], [3, 169, 244], [0, 188, 212], [20, 55, 200],\n",
        "                   [0, 150, 136], [76, 175, 80], [139, 195, 74], [205, 220, 57], [70, 25, 100],\n",
        "                   [255, 235, 59], [255, 193, 7], [255, 152, 0], [255, 87, 34], [90, 155, 50],\n",
        "                   [121, 85, 72], [158, 158, 158], [96, 125, 139], [15, 67, 34], [98, 55, 20],\n",
        "                   [21, 82, 172], [58, 128, 255], [196, 125, 39], [75, 27, 134], [90, 125, 120],\n",
        "                   [121, 82, 7], [158, 58, 8], [96, 25, 9], [115, 7, 234], [8, 155, 220],\n",
        "                   [221, 25, 72], [188, 58, 158], [56, 175, 19], [215, 67, 64], [198, 75, 20],\n",
        "                   [62, 185, 22], [108, 70, 58], [160, 225, 39], [95, 60, 144], [78, 155, 120],\n",
        "                   [101, 25, 142], [48, 198, 28], [96, 225, 200], [150, 167, 134], [18, 185, 90],\n",
        "                   [21, 145, 172], [98, 68, 78], [196, 105, 19], [215, 67, 84], [130, 115, 170],\n",
        "                   [255, 0, 255], [255, 255, 0], [196, 185, 10], [95, 167, 234], [18, 25, 190],\n",
        "                   [0, 255, 255], [255, 0, 0], [0, 255, 0], [0, 0, 255], [155, 0, 0],\n",
        "                   [0, 155, 0], [0, 0, 155], [46, 22, 130], [255, 0, 155], [155, 0, 255],\n",
        "                   [255, 155, 0], [155, 255, 0], [0, 155, 255], [0, 255, 155], [18, 5, 40],\n",
        "                   [120, 120, 255], [255, 58, 30], [60, 45, 60], [75, 27, 244], [128, 25, 70]], dtype='uint8')\n",
        "\n",
        "COCO_LABEL_MAP = {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8,\n",
        "                  9: 9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16,\n",
        "                  18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24,\n",
        "                  27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32,\n",
        "                  37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40,\n",
        "                  46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48,\n",
        "                  54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56,\n",
        "                  62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64,\n",
        "                  74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72,\n",
        "                  82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n",
        "\n",
        "CUSTOM_CLASSES = ('cells','kernel')\n",
        "# CUSTOM_CLASSES = ('dog', 'person', 'bear', 'sheep')\n",
        "\n",
        "norm_mean = np.array([103.94, 116.78, 123.68], dtype=np.float32)\n",
        "norm_std = np.array([57.38, 57.12, 58.40], dtype=np.float32)\n",
        "\n",
        "class res101_coco:\n",
        "    def __init__(self, args):\n",
        "        self.mode = args.mode\n",
        "        self.cuda = args.cuda\n",
        "        self.gpu_id = 0\n",
        "        assert args.img_size % 32 == 0, f'Img_size must be divisible by 32, got {args.img_size}.'\n",
        "        self.img_size = args.img_size\n",
        "        self.class_names = CUSTOM_CLASSES\n",
        "        self.num_classes = len(CUSTOM_CLASSES) + 1\n",
        "        self.continuous_id = {(aa + 1): (aa + 1) for aa in range(self.num_classes - 1)}\n",
        "        self.scales = [int(self.img_size / 544 * aa) for aa in (24, 48, 96, 192, 384)]\n",
        "        self.aspect_ratios = [1, 1 / 2, 2]\n",
        "\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.weight = args.resume if args.resume else '/content/drive/MyDrive/yolact/weights/backbone_res101.pth'\n",
        "            # if not args.weight:\n",
        "            #     # net.apply(init_weights)\n",
        "            #     self.weight = init_weights(net)\n",
        "        else:\n",
        "            self.weight = args.weight\n",
        "\n",
        "        self.data_root = '/content/drive/MyDrive/'\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.train_imgs = self.data_root + 'custom_dataset/train/'\n",
        "            self.train_ann = self.data_root + 'custom_dataset/train/_annotations.coco.json'\n",
        "            self.train_bs = args.train_bs\n",
        "            self.bs_per_gpu = args.bs_per_gpu\n",
        "            self.val_interval = args.val_interval\n",
        "\n",
        "            self.bs_factor = self.train_bs / 8\n",
        "            self.lr = 0.001 * self.bs_factor\n",
        "            # self.lr = 0.001\n",
        "            self.warmup_init = self.lr * 0.1\n",
        "            self.warmup_until = 500  # If adapted with bs_factor, inifinte loss may appear.\n",
        "            self.lr_steps = tuple([int(aa / self.bs_factor) for aa in (0, 280000, 560000, 620000, 680000)])\n",
        "\n",
        "            self.pos_iou_thre = 0.5\n",
        "            self.neg_iou_thre = 0.4\n",
        "\n",
        "            self.conf_alpha = 1\n",
        "            self.bbox_alpha = 1.5\n",
        "            self.mask_alpha = 6.125\n",
        "            self.semantic_alpha = 1\n",
        "\n",
        "            # The max number of masks to train for one image.\n",
        "            self.masks_to_train = 200\n",
        "\n",
        "        if self.mode in ('train', 'val'):\n",
        "            self.val_imgs = self.data_root + 'custom_dataset/valid/'\n",
        "            self.val_ann = self.data_root + 'custom_dataset/valid/_annotations.coco.json'\n",
        "            self.val_bs = 1\n",
        "            self.val_num = args.val_num\n",
        "            self.coco_api = args.coco_api\n",
        "\n",
        "        self.traditional_nms = args.traditional_nms\n",
        "        self.nms_score_thre = 0.05\n",
        "        self.nms_iou_thre = 0.5\n",
        "        self.top_k = 200\n",
        "        self.max_detections = 200\n",
        "\n",
        "        if self.mode == 'detect':\n",
        "            for k, v in vars(args).items():\n",
        "                self.__setattr__(k, v)\n",
        "\n",
        "    def print_cfg(self):\n",
        "        print()\n",
        "        print('-' * 30 + self.__class__.__name__ + '-' * 30)\n",
        "        for k, v in vars(self).items():\n",
        "            if k not in ('continuous_id', 'data_root', 'cfg'):\n",
        "                print(f'{k}: {v}')\n",
        "        print()\n",
        "\n",
        "\n",
        "def get_config(args, mode):\n",
        "    args.cuda = torch.cuda.is_available()\n",
        "    args.mode = mode\n",
        "\n",
        "    if args.cuda:\n",
        "        # Set GPU id if CUDA is available\n",
        "        # args.gpu_id = '0'  # Assuming using the first GPU\n",
        "        if args.mode == 'train':\n",
        "            # Training mode\n",
        "            num_gpus = torch.cuda.device_count()\n",
        "            args.bs_per_gpu = args.train_bs // num_gpus\n",
        "        # else:\n",
        "            # Validation or detection mode\n",
        "            # assert args.gpu_id.isdigit(), 'Only one GPU can be used in val/detect mode.'\n",
        "    else:\n",
        "        # args.gpu_id = None\n",
        "        if args.mode == 'train':\n",
        "            args.bs_per_gpu = args.train_bs\n",
        "            print('\\n-----No GPU found, training on CPU.-----')\n",
        "        else:\n",
        "            print('\\n-----No GPU found, validate on CPU.-----')\n",
        "\n",
        "    # klas = 'res101_coco'\n",
        "    # cfg = globals()[args.klas](args)\n",
        "    cfg = res101_coco(args)\n",
        "\n",
        "    # Print configuration\n",
        "    if not args.cuda or args.mode != 'train':\n",
        "        cfg.print_cfg()\n",
        "    else:\n",
        "        cfg.print_cfg()\n",
        "\n",
        "    return cfg"
      ],
      "metadata": {
        "id": "yRNJuyZM3kYo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyximport\n",
        "pyximport.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQAK-cJN3zBG",
        "outputId": "e0675f77-a73a-4eb4-d648-b15f50fd262b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, <pyximport._pyximport3.PyxImportMetaFinder at 0x7e3eceb752d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cythonize -a cython_nms.pyx"
      ],
      "metadata": {
        "id": "VTw4UYbq4Hs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3217aa5c-09e1-46e0-ce0b-2c44885e77f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/cythonize: No such file or directory: 'cython_nms.pyx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Warning, do not use numpy random in PyTorch multiprocessing, or the random result will be the same.\n",
        "\n",
        "def random_mirror(img, masks, boxes):\n",
        "    if random.randint(0, 1):\n",
        "        _, width, _ = img.shape\n",
        "        img = img[:, ::-1]\n",
        "        masks = masks[:, :, ::-1]\n",
        "        boxes[:, 0::2] = width - boxes[:, 2::-2]\n",
        "\n",
        "    return img, masks, boxes\n",
        "\n",
        "\n",
        "def clip_box(hw, boxes):\n",
        "    boxes[:, [0, 2]] = np.clip(boxes[:, [0, 2]], a_min=0, a_max=hw[1] - 1)\n",
        "    boxes[:, [1, 3]] = np.clip(boxes[:, [1, 3]], a_min=0, a_max=hw[0] - 1)\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def remove_small_box(boxes, masks, labels, area_limit):\n",
        "    box_areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "    keep = box_areas > area_limit\n",
        "    return boxes[keep], masks[keep], labels[keep]\n",
        "\n",
        "\n",
        "def to_01_box(hw, boxes):\n",
        "    boxes[:, [0, 2]] /= hw[1]\n",
        "    boxes[:, [1, 3]] /= hw[0]\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def random_brightness(img, delta=32):\n",
        "    img += random.uniform(-delta, delta)\n",
        "\n",
        "    return np.clip(img, 0., 255.)\n",
        "\n",
        "\n",
        "def random_contrast(img, lower=0.7, upper=1.3):\n",
        "    img *= random.uniform(lower, upper)\n",
        "\n",
        "    return np.clip(img, 0., 255.)\n",
        "\n",
        "\n",
        "def random_saturation(img, lower=0.7, upper=1.3):\n",
        "    img[:, :, 1] *= random.uniform(lower, upper)\n",
        "    return img\n",
        "\n",
        "\n",
        "def random_hue(img, delta=15.):  # better keep 0.< delta <=30., two large delta harms the image\n",
        "    img[:, :, 0] += random.uniform(-delta, delta)\n",
        "    img[:, :, 0][img[:, :, 0] > 360.0] -= 360.0\n",
        "    img[:, :, 0][img[:, :, 0] < 0.0] += 360.0\n",
        "    return img\n",
        "\n",
        "\n",
        "def photometric_distort(img):\n",
        "    if random.randint(0, 1):\n",
        "        img = random_brightness(img)\n",
        "    if random.randint(0, 1):\n",
        "        img = random_contrast(img)\n",
        "\n",
        "    # Because of normalization, random brightness and random contrast are meanless\n",
        "    # if random_saturation and random_hue do not follow.\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    img = random_saturation(img)\n",
        "    img = random_hue(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n",
        "    img = np.clip(img, 0., 255.)\n",
        "\n",
        "    # TODO: need a random noise aug crop\n",
        "    return img\n",
        "\n",
        "\n",
        "def crop_eight(ori_h, crop_h, ori_w, crop_w, img, masks, boxes, labels, keep_ratio=0.3):\n",
        "    num_boxes = boxes.shape[0]\n",
        "    box_x1, box_y1, box_x2, box_y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "    box_areas = (box_x2 - box_x1) * (box_y2 - box_y1)\n",
        "\n",
        "    ii = 0\n",
        "    cut_out = True\n",
        "    while cut_out:\n",
        "        ii += 1\n",
        "        if ii > 1000:\n",
        "            return None, None, None, None\n",
        "\n",
        "        random_x1 = random.randint(0, ori_w - crop_w)\n",
        "        random_y1 = random.randint(0, ori_h - crop_h)\n",
        "\n",
        "        new_x1 = np.tile(random_x1, (num_boxes, 1)).astype('float32')\n",
        "        new_y1 = np.tile(random_y1, (num_boxes, 1)).astype('float32')\n",
        "        new_x2 = np.tile(random_x1 + crop_w, (num_boxes, 1)).astype('float32')\n",
        "        new_y2 = np.tile(random_y1 + crop_h, (num_boxes, 1)).astype('float32')\n",
        "\n",
        "        min_x1 = np.max(np.concatenate([new_x1, box_x1.reshape(num_boxes, -1)], axis=1), axis=1)\n",
        "        min_y1 = np.max(np.concatenate([new_y1, box_y1.reshape(num_boxes, -1)], axis=1), axis=1)\n",
        "        max_x2 = np.min(np.concatenate([new_x2, box_x2.reshape(num_boxes, -1)], axis=1), axis=1)\n",
        "        max_y2 = np.min(np.concatenate([new_y2, box_y2.reshape(num_boxes, -1)], axis=1), axis=1)\n",
        "\n",
        "        inter_w = np.clip((max_x2 - min_x1), a_min=0, a_max=10000)\n",
        "        inter_h = np.clip((max_y2 - min_y1), a_min=0, a_max=10000)\n",
        "\n",
        "        inter_area = inter_w * inter_h\n",
        "        keep = (inter_area / box_areas) > keep_ratio\n",
        "\n",
        "        if keep.any():\n",
        "            box_part = np.stack([min_x1, min_y1, max_x2, max_y2, labels], axis=0).T\n",
        "            boxes_remained = box_part[keep]\n",
        "            masks_remained = masks[keep]\n",
        "\n",
        "            boxes_remained[:, [0, 2]] -= random_x1\n",
        "            boxes_remained[:, [1, 3]] -= random_y1\n",
        "\n",
        "            img_cropped = img[int(new_y1[0]): int(new_y2[0]), int(new_x1[0]): int(new_x2[0]), :]\n",
        "            masks_remained = masks_remained[:, int(new_y1[0]): int(new_y2[0]), int(new_x1[0]): int(new_x2[0])]\n",
        "\n",
        "            cut_out = False\n",
        "\n",
        "    return img_cropped, masks_remained, boxes_remained[:, :4], boxes_remained[:, 4]\n",
        "\n",
        "\n",
        "def random_crop(img, masks, boxes, labels, crop_ratio):\n",
        "    if random.randint(0, 1):\n",
        "        return img, masks, boxes, labels\n",
        "    else:\n",
        "        ori_h, ori_w, _ = img.shape\n",
        "        crop_h = int(random.uniform(crop_ratio[0], crop_ratio[1]) * ori_h)\n",
        "        crop_w = int(random.uniform(crop_ratio[0], crop_ratio[1]) * ori_w)\n",
        "\n",
        "        return crop_eight(ori_h, crop_h, ori_w, crop_w, img, masks, boxes, labels)\n",
        "\n",
        "\n",
        "def pad_to_square(img, masks=None, boxes=None, during_training=False):\n",
        "    img_h, img_w = img.shape[:2]\n",
        "    if img_h == img_w:\n",
        "        return (img, masks, boxes) if during_training else img\n",
        "    else:\n",
        "        pad_size = max(img_h, img_w)\n",
        "        pad_img = np.zeros((pad_size, pad_size, 3), dtype='float32')\n",
        "        pad_img[:, :, :] = norm_mean\n",
        "\n",
        "        if during_training:\n",
        "            pad_masks = np.zeros((masks.shape[0], pad_size, pad_size), dtype='float32')\n",
        "\n",
        "            if img_h < img_w:\n",
        "                random_y1 = random.randint(0, img_w - img_h)\n",
        "                pad_img[random_y1: random_y1 + img_h, :, :] = img\n",
        "                pad_masks[:, random_y1: random_y1 + img_h, :] = masks\n",
        "                boxes[:, [1, 3]] += random_y1\n",
        "\n",
        "            if img_h > img_w:\n",
        "                random_x1 = random.randint(0, img_h - img_w)\n",
        "                pad_img[:, random_x1: random_x1 + img_w, :] = img\n",
        "                pad_masks[:, :, random_x1: random_x1 + img_w] = masks\n",
        "                boxes[:, [0, 2]] += random_x1\n",
        "\n",
        "            return pad_img, pad_masks, boxes\n",
        "        else:\n",
        "            pad_img[0: img_h, 0: img_w, :] = img\n",
        "            return pad_img\n",
        "\n",
        "\n",
        "def multi_scale_resize(img, masks=None, boxes=None, resize_range=None, during_training=False):\n",
        "    assert img.shape[0] == img.shape[1], 'Error, image is not square in <multi_scale_resize>'\n",
        "\n",
        "    if during_training:\n",
        "        ori_size = img.shape[0]\n",
        "        resize_size = random.randint(resize_range[0], resize_range[1]) * 32\n",
        "        img = cv2.resize(img, (resize_size, resize_size))\n",
        "        scale = resize_size / ori_size\n",
        "        boxes *= scale\n",
        "\n",
        "        masks = masks.transpose((1, 2, 0))\n",
        "        masks = cv2.resize(masks, (resize_size, resize_size))\n",
        "\n",
        "        # OpenCV resizes a (w,h,1) array to (s,s), so fix it\n",
        "        if len(masks.shape) == 2:\n",
        "            masks = np.expand_dims(masks, 0)\n",
        "        else:\n",
        "            masks = masks.transpose((2, 0, 1))\n",
        "\n",
        "        return img, masks, boxes\n",
        "    else:\n",
        "        return cv2.resize(img, (resize_range, resize_range))\n",
        "\n",
        "\n",
        "def to_train_size(img, masks, boxes, labels, train_size):\n",
        "    img_size = img.shape[0]\n",
        "\n",
        "    if img_size == train_size:\n",
        "        return img, masks, boxes, labels\n",
        "    elif img_size < train_size:\n",
        "        # pad_img = np.zeros((train_size, train_size, 3), dtype='float32')\n",
        "        # pad_masks = np.zeros((masks.shape[0], train_size, train_size), dtype='float32')\n",
        "        # pad_img[:, :, :] = norm_mean\n",
        "        # random_y1 = random.randint(0, train_size - img_size)\n",
        "        # random_x1 = random.randint(0, train_size - img_size)\n",
        "        # pad_img[random_y1: random_y1 + img_size, random_x1: random_x1 + img_size, :] = img\n",
        "        # pad_masks[:, random_y1: random_y1 + img_size, random_x1: random_x1 + img_size] = masks\n",
        "        # boxes[:, [1, 3]] += random_y1\n",
        "        # boxes[:, [0, 2]] += random_x1\n",
        "        # return pad_img, pad_masks, boxes, labels\n",
        "        # crop\n",
        "        pass\n",
        "    else:\n",
        "        return crop_eight(img_size, train_size, img_size, train_size, img, masks, boxes, labels)\n",
        "\n",
        "\n",
        "def normalize_and_toRGB(img):\n",
        "    img = (img - norm_mean) / norm_std\n",
        "    img = img[:, :, (2, 1, 0)]\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    return img\n",
        "\n",
        "\n",
        "def val_aug(img, val_size):\n",
        "    img = img.astype('float32')\n",
        "    img = pad_to_square(img, during_training=False)\n",
        "    img = multi_scale_resize(img, resize_range=val_size, during_training=False)\n",
        "    img = normalize_and_toRGB(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def train_aug(img, masks, boxes, labels, train_size):\n",
        "    # show_ann(img, masks, boxes, labels)\n",
        "    img = img.astype('float32')\n",
        "    img = photometric_distort(img)\n",
        "    img, masks, boxes = random_mirror(img, masks, boxes)\n",
        "    img, masks, boxes, labels = random_crop(img, masks, boxes, labels, crop_ratio=(0.6, 1))\n",
        "    if img is None:\n",
        "        return None, None, None, None\n",
        "    img, masks, boxes = pad_to_square(img, masks, boxes, during_training=True)\n",
        "    img, masks, boxes = multi_scale_resize(img, masks, boxes, (8, 24), during_training=True)  # multiple of 32\n",
        "    img, masks, boxes, labels = to_train_size(img, masks, boxes, labels, train_size)\n",
        "    if img is None:\n",
        "        return None, None, None, None\n",
        "    boxes = clip_box(img.shape[:2], boxes)\n",
        "    boxes, masks, labels = remove_small_box(boxes, masks, labels, area_limit=20)\n",
        "    if boxes.shape[0] == 0:\n",
        "        return None, None, None, None\n",
        "    assert boxes.shape[0] == masks.shape[0] == labels.shape[0], 'Error, unequal boxes, masks or labels number.'\n",
        "    # show_ann(img, masks, boxes, labels)\n",
        "    boxes = to_01_box(img.shape[:2], boxes)\n",
        "    img = normalize_and_toRGB(img)\n",
        "\n",
        "    return img, masks, boxes, labels\n",
        "\n",
        "\n",
        "def show_ann(img, masks, boxes, labels):\n",
        "    masks_semantic = masks * (labels[:, None, None] + 1)  # expand class_ids' shape for broadcasting\n",
        "    # The color of the overlap area is different because of the '%' operation.\n",
        "    masks_semantic = masks_semantic.astype('int').sum(axis=0) % 80\n",
        "    color_masks = COLORS[masks_semantic].astype('uint8')\n",
        "    img_u8 = img.astype('uint8')\n",
        "    img_fused = cv2.addWeighted(color_masks, 0.4, img_u8, 0.6, gamma=0)\n",
        "\n",
        "    for i in range(boxes.shape[0]):\n",
        "        cv2.rectangle(img_fused, (int(boxes[i, 0]), int(boxes[i, 1])),\n",
        "                      (int(boxes[i, 2]), int(boxes[i, 3])), (0, 255, 0), 1)\n",
        "\n",
        "    print(f'\\nimg shape: {img.shape}')\n",
        "    print('----------------boxes----------------')\n",
        "    print(boxes)\n",
        "    print('----------------labels---------------')\n",
        "    print(labels, '\\n')\n",
        "    cv2.imshow('aa', img_fused)\n",
        "    cv2.waitKey()\n"
      ],
      "metadata": {
        "id": "sc58JiNZ5S45"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "from itertools import product\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def box_iou(box_a, box_b):\n",
        "    \"\"\"\n",
        "    Compute the IoU of two sets of boxes.\n",
        "    Args:\n",
        "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
        "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
        "    Return:\n",
        "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
        "    \"\"\"\n",
        "    use_batch = True\n",
        "    if box_a.dim() == 2:\n",
        "        use_batch = False\n",
        "        box_a = box_a[None, ...]\n",
        "        box_b = box_b[None, ...]\n",
        "\n",
        "    (n, A), B = box_a.shape[:2], box_b.shape[1]\n",
        "    # add a dimension\n",
        "    box_a = box_a[:, :, None, :].expand(n, A, B, 4)\n",
        "    box_b = box_b[:, None, :, :].expand(n, A, B, 4)\n",
        "\n",
        "    max_xy = torch.min(box_a[..., 2:], box_b[..., 2:])\n",
        "    min_xy = torch.max(box_a[..., :2], box_b[..., :2])\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    inter_area = inter[..., 0] * inter[..., 1]\n",
        "\n",
        "    area_a = (box_a[..., 2] - box_a[..., 0]) * (box_a[..., 3] - box_a[..., 1])\n",
        "    area_b = (box_b[..., 2] - box_b[..., 0]) * (box_b[..., 3] - box_b[..., 1])\n",
        "\n",
        "    out = inter_area / (area_a + area_b - inter_area)\n",
        "    return out if use_batch else out.squeeze(0)\n",
        "\n",
        "\n",
        "def box_iou_numpy(box_a, box_b):\n",
        "    (n, A), B = box_a.shape[:2], box_b.shape[1]\n",
        "    # add a dimension\n",
        "    box_a = np.tile(box_a[:, :, None, :], (1, 1, B, 1))\n",
        "    box_b = np.tile(box_b[:, None, :, :], (1, A, 1, 1))\n",
        "\n",
        "    max_xy = np.minimum(box_a[..., 2:], box_b[..., 2:])\n",
        "    min_xy = np.maximum(box_a[..., :2], box_b[..., :2])\n",
        "    inter = np.clip((max_xy - min_xy), a_min=0, a_max=100000)\n",
        "    inter_area = inter[..., 0] * inter[..., 1]\n",
        "\n",
        "    area_a = (box_a[..., 2] - box_a[..., 0]) * (box_a[..., 3] - box_a[..., 1])\n",
        "    area_b = (box_b[..., 2] - box_b[..., 0]) * (box_b[..., 3] - box_b[..., 1])\n",
        "\n",
        "    return inter_area / (area_a + area_b - inter_area)\n",
        "\n",
        "\n",
        "def match(cfg, box_gt, anchors, class_gt):\n",
        "    # Convert prior boxes to the form of [xmin, ymin, xmax, ymax].\n",
        "    decoded_priors = torch.cat((anchors[:, :2] - anchors[:, 2:] / 2, anchors[:, :2] + anchors[:, 2:] / 2), 1)\n",
        "\n",
        "    overlaps = box_iou(box_gt, decoded_priors)  # (num_gts, num_achors)\n",
        "\n",
        "    _, gt_max_i = overlaps.max(1)  # (num_gts, ) the max IoU for each gt box\n",
        "    each_anchor_max, anchor_max_i = overlaps.max(0)  # (num_achors, ) the max IoU for each anchor\n",
        "\n",
        "    each_anchor_max.index_fill_(0, gt_max_i, 2)\n",
        "\n",
        "    # Set the index of the pair (anchor, gt) we set the overlap for above.\n",
        "    for j in range(gt_max_i.size(0)):\n",
        "        anchor_max_i[gt_max_i[j]] = j\n",
        "\n",
        "    anchor_max_gt = box_gt[anchor_max_i]  # (num_achors, 4)\n",
        "\n",
        "    conf = class_gt[anchor_max_i] + 1  # the class of the max IoU gt box for each anchor\n",
        "    conf[each_anchor_max < cfg.pos_iou_thre] = -1  # label as neutral\n",
        "    conf[each_anchor_max < cfg.neg_iou_thre] = 0  # label as background\n",
        "\n",
        "    offsets = encode(anchor_max_gt, anchors)\n",
        "\n",
        "    return offsets, conf, anchor_max_gt, anchor_max_i\n",
        "\n",
        "\n",
        "def make_anchors(cfg, conv_h, conv_w, scale):\n",
        "    prior_data = []\n",
        "    # Iteration order is important (it has to sync up with the convout)\n",
        "    for j, i in product(range(conv_h), range(conv_w)):\n",
        "        # + 0.5 because priors are in center\n",
        "        x = (i + 0.5) / conv_w\n",
        "        y = (j + 0.5) / conv_h\n",
        "\n",
        "        for ar in cfg.aspect_ratios:\n",
        "            ar = sqrt(ar)\n",
        "            w = scale * ar / cfg.img_size\n",
        "            h = scale / ar / cfg.img_size\n",
        "\n",
        "            prior_data += [x, y, w, h]\n",
        "\n",
        "    return prior_data\n",
        "\n",
        "\n",
        "def encode(matched, priors):\n",
        "    variances = [0.1, 0.2]\n",
        "\n",
        "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]  # 10 * (Xg - Xa) / Wa\n",
        "    g_cxcy /= (variances[0] * priors[:, 2:])  # 10 * (Yg - Ya) / Ha\n",
        "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]  # 5 * log(Wg / Wa)\n",
        "    g_wh = torch.log(g_wh) / variances[1]  # 5 * log(Hg / Ha)\n",
        "    # return target for smooth_l1_loss\n",
        "    offsets = torch.cat([g_cxcy, g_wh], 1)  # [num_priors, 4]\n",
        "\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def sanitize_coordinates(_x1, _x2, img_size, padding=0):\n",
        "\n",
        "    _x1 = _x1 * img_size\n",
        "    _x2 = _x2 * img_size\n",
        "\n",
        "    x1 = torch.min(_x1, _x2)\n",
        "    x2 = torch.max(_x1, _x2)\n",
        "    x1 = torch.clamp(x1 - padding, min=0)\n",
        "    x2 = torch.clamp(x2 + padding, max=img_size)\n",
        "\n",
        "    return x1, x2\n",
        "\n",
        "\n",
        "def sanitize_coordinates_numpy(_x1, _x2, img_size, padding=0):\n",
        "    _x1 = _x1 * img_size\n",
        "    _x2 = _x2 * img_size\n",
        "\n",
        "    x1 = np.minimum(_x1, _x2)\n",
        "    x2 = np.maximum(_x1, _x2)\n",
        "    x1 = np.clip(x1 - padding, a_min=0, a_max=1000000)\n",
        "    x2 = np.clip(x2 + padding, a_min=0, a_max=img_size)\n",
        "\n",
        "    return x1, x2\n",
        "\n",
        "\n",
        "def crop_box(masks, boxes, padding=1):\n",
        "\n",
        "    h, w, n = masks.size()\n",
        "    x1, x2 = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, padding)\n",
        "    y1, y2 = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, padding)\n",
        "\n",
        "    rows = torch.arange(w, device=masks.device, dtype=x1.dtype).view(1, -1, 1).expand(h, w, n)\n",
        "    cols = torch.arange(h, device=masks.device, dtype=x1.dtype).view(-1, 1, 1).expand(h, w, n)\n",
        "\n",
        "    masks_left = rows >= x1.view(1, 1, -1)\n",
        "    masks_right = rows < x2.view(1, 1, -1)\n",
        "    masks_up = cols >= y1.view(1, 1, -1)\n",
        "    masks_down = cols < y2.view(1, 1, -1)\n",
        "\n",
        "    crop_mask = masks_left * masks_right * masks_up * masks_down\n",
        "\n",
        "    return masks * crop_mask.float()\n",
        "\n",
        "\n",
        "def crop_numpy(masks, boxes, padding=1):\n",
        "    h, w, n = masks.shape\n",
        "    x1, x2 = sanitize_coordinates_numpy(boxes[:, 0], boxes[:, 2], w, padding)\n",
        "    y1, y2 = sanitize_coordinates_numpy(boxes[:, 1], boxes[:, 3], h, padding)\n",
        "\n",
        "    rows = np.tile(np.arange(w)[None, :, None], (h, 1, n))\n",
        "    cols = np.tile(np.arange(h)[:, None, None], (1, w, n))\n",
        "\n",
        "    masks_left = rows >= (x1.reshape(1, 1, -1))\n",
        "    masks_right = rows < (x2.reshape(1, 1, -1))\n",
        "    masks_up = cols >= (y1.reshape(1, 1, -1))\n",
        "    masks_down = cols < (y2.reshape(1, 1, -1))\n",
        "\n",
        "    crop_mask = masks_left * masks_right * masks_up * masks_down\n",
        "\n",
        "    return masks * crop_mask\n",
        "\n",
        "\n",
        "def mask_iou(mask1, mask2):\n",
        "\n",
        "    intersection = torch.matmul(mask1, mask2.t())\n",
        "    area1 = torch.sum(mask1, dim=1).reshape(1, -1)\n",
        "    area2 = torch.sum(mask2, dim=1).reshape(1, -1)\n",
        "    union = (area1.t() + area2) - intersection\n",
        "    ret = intersection / union\n",
        "\n",
        "    return ret.cpu()\n"
      ],
      "metadata": {
        "id": "XicuMnif5ziR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "\n",
        "def train_collate(batch):\n",
        "    imgs, targets, masks = [], [], []\n",
        "    valid_batch = [aa for aa in batch if aa[0] is not None]\n",
        "\n",
        "    lack_len = len(batch) - len(valid_batch)\n",
        "    if lack_len > 0:\n",
        "        for i in range(lack_len):\n",
        "            valid_batch.append(valid_batch[i])\n",
        "\n",
        "    for sample in valid_batch:\n",
        "        imgs.append(torch.tensor(sample[0], dtype=torch.float32))\n",
        "        targets.append(torch.tensor(sample[1], dtype=torch.float32))\n",
        "        masks.append(torch.tensor(sample[2], dtype=torch.float32))\n",
        "\n",
        "    return torch.stack(imgs, 0), targets, masks\n",
        "\n",
        "\n",
        "def val_collate(batch):\n",
        "    imgs = torch.tensor(batch[0][0], dtype=torch.float32).unsqueeze(0)\n",
        "    targets = torch.tensor(batch[0][1], dtype=torch.float32)\n",
        "    masks = torch.tensor(batch[0][2], dtype=torch.float32)\n",
        "    return imgs, targets, masks, batch[0][3], batch[0][4]\n",
        "\n",
        "\n",
        "def detect_collate(batch):\n",
        "    imgs = torch.tensor(batch[0][0], dtype=torch.float32).unsqueeze(0)\n",
        "    return imgs, batch[0][1], batch[0][2]\n",
        "\n",
        "\n",
        "def detect_onnx_collate(batch):\n",
        "    return batch[0][0][None, :], batch[0][1], batch[0][2]\n",
        "\n",
        "\n",
        "class COCODetection(data.Dataset):\n",
        "    def __init__(self, cfg, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.cfg = cfg\n",
        "\n",
        "        if mode in ('train', 'val'):\n",
        "            self.image_path = cfg.train_imgs if mode == 'train' else cfg.val_imgs\n",
        "            self.coco = COCO(cfg.train_ann if mode == 'train' else cfg.val_ann)\n",
        "            self.ids = list(self.coco.imgToAnns.keys())\n",
        "        elif mode == 'detect':\n",
        "            self.image_path = glob.glob(cfg.image + '/*.jpg')\n",
        "            self.image_path.sort()\n",
        "\n",
        "        self.continuous_id = cfg.continuous_id\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == 'detect':\n",
        "            img_name = self.image_path[index]\n",
        "            img_origin = cv2.imread(img_name)\n",
        "            img_normed = val_aug(img_origin, self.cfg.img_size)\n",
        "            return img_normed, img_origin, img_name.split(osp.sep)[-1]\n",
        "        else:\n",
        "            img_id = self.ids[index]\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "\n",
        "            # 'target' includes {'segmentation', 'area', iscrowd', 'image_id', 'bbox', 'category_id'}\n",
        "            target = self.coco.loadAnns(ann_ids)\n",
        "            target = [aa for aa in target if not aa['iscrowd']]\n",
        "\n",
        "            file_name = self.coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "            img_path = osp.join(self.image_path, file_name)\n",
        "            assert osp.exists(img_path), f'Image path does not exist: {img_path}'\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            height, width, _ = img.shape\n",
        "\n",
        "            assert len(target) > 0, 'No annotation in this image!'\n",
        "            box_list, mask_list, label_list = [], [], []\n",
        "\n",
        "            for aa in target:\n",
        "                bbox = aa['bbox']\n",
        "\n",
        "                # When training, some boxes are wrong, ignore them.\n",
        "                if self.mode == 'train':\n",
        "                    if bbox[0] < 0 or bbox[1] < 0 or bbox[2] < 4 or bbox[3] < 4:\n",
        "                        continue\n",
        "\n",
        "                x1y1x2y2_box = np.array([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
        "                category = self.continuous_id[aa['category_id']] - 1\n",
        "\n",
        "                box_list.append(x1y1x2y2_box)\n",
        "                mask_list.append(self.coco.annToMask(aa))\n",
        "                label_list.append(category)\n",
        "\n",
        "            if len(box_list) > 0:\n",
        "                boxes = np.array(box_list)\n",
        "                masks = np.stack(mask_list, axis=0)\n",
        "                labels = np.array(label_list)\n",
        "                assert masks.shape == (boxes.shape[0], height, width), 'Unmatched annotations.'\n",
        "\n",
        "                if self.mode == 'train':\n",
        "                    img, masks, boxes, labels = train_aug(img, masks, boxes, labels, self.cfg.img_size)\n",
        "                    if img is None:\n",
        "                        return None, None, None\n",
        "                    else:\n",
        "                        boxes = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "                        return img, boxes, masks\n",
        "                elif self.mode == 'val':\n",
        "                    img = val_aug(img, self.cfg.img_size)\n",
        "                    boxes = boxes / np.array([width, height, width, height])  # to 0~1 scale\n",
        "                    boxes = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "                    return img, boxes, masks, height, width\n",
        "            else:\n",
        "                if self.mode == 'val':\n",
        "                    raise RuntimeError('Error, no valid object in this image.')\n",
        "                else:\n",
        "                    print(f'No valid object in image: {img_id}. Use a repeated image in this batch.')\n",
        "                    return None, None, None\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == 'train':\n",
        "            return len(self.ids)\n",
        "        elif self.mode == 'val':\n",
        "            return len(self.ids) if self.cfg.val_num == -1 else min(self.cfg.val_num, len(self.ids))\n",
        "        elif self.mode == 'detect':\n",
        "            return len(self.image_path)\n"
      ],
      "metadata": {
        "id": "RvqMGYFh55fb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install terminaltables\n",
        "import glob\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pycocotools\n",
        "import json\n",
        "from terminaltables import AsciiTable\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class ProgressBar:\n",
        "    def __init__(self, length, max_val):\n",
        "        self.max_val = max_val\n",
        "        self.length = length\n",
        "        self.cur_val = 0\n",
        "\n",
        "        self.cur_num_bars = -1\n",
        "        self.update_str()\n",
        "\n",
        "    def update_str(self):\n",
        "        num_bars = int(self.length * (self.cur_val / self.max_val))\n",
        "\n",
        "        if num_bars != self.cur_num_bars:\n",
        "            self.cur_num_bars = num_bars\n",
        "            self.string = '█' * num_bars + '░' * (self.length - num_bars)\n",
        "\n",
        "    def get_bar(self, new_val):\n",
        "        self.cur_val = new_val\n",
        "\n",
        "        if self.cur_val > self.max_val:\n",
        "            self.cur_val = self.max_val\n",
        "        self.update_str()\n",
        "        return self.string\n",
        "\n",
        "\n",
        "def save_best(net, mask_map, cfg_name, step):\n",
        "    weight = glob.glob('/content/drive/MyDrive/yolact/weights/best*')\n",
        "    weight = [aa for aa in weight if cfg_name in aa]\n",
        "    assert len(weight) <= 1, 'Error, multiple best weight found.'\n",
        "    best_mask_map = float(weight[0].split('/')[-1].split('_')[1]) if weight else 0.\n",
        "\n",
        "    if mask_map >= best_mask_map:\n",
        "        if weight:\n",
        "            os.remove(weight[0])  # remove the last best model\n",
        "\n",
        "        print(f'\\nSaving the best model as \\'best_{mask_map}_{cfg_name}_{step}.pth\\'.\\n')\n",
        "        torch.save(net.state_dict(), f'/content/drive/MyDrive/yolact/weights/best_{mask_map}_{cfg_name}_{step}.pth')\n",
        "\n",
        "\n",
        "def save_latest(net, cfg_name, step):\n",
        "    weight = glob.glob('/content/drive/MyDrive/yolact/weights/latest*')\n",
        "    weight = [aa for aa in weight if cfg_name in aa]\n",
        "    assert len(weight) <= 1, 'Error, multiple latest weight found.'\n",
        "    if weight:\n",
        "        os.remove(weight[0])\n",
        "\n",
        "    print(f'\\nSaving the latest model as \\'latest_{cfg_name}_{step}.pth\\'.\\n')\n",
        "    torch.save(net.state_dict(), f'/content/drive/MyDrive/yolact/weights/latest_{cfg_name}_{step}.pth')\n",
        "\n",
        "\n",
        "class MakeJson:\n",
        "    def __init__(self):\n",
        "        self.bbox_data = []\n",
        "        self.mask_data = []\n",
        "        self.coco_cats = {}\n",
        "\n",
        "        for coco_id, real_id in COCO_LABEL_MAP.items():\n",
        "            class_id = real_id - 1\n",
        "            self.coco_cats[class_id] = coco_id\n",
        "\n",
        "    def add_bbox(self, image_id: int, category_id: int, bbox: list, score: float):\n",
        "        \"\"\" Note that bbox should be a list or tuple of (x1, y1, x2, y2) \"\"\"\n",
        "        bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
        "\n",
        "        # Round to the nearest 10th to avoid huge file sizes, as COCO suggests\n",
        "        bbox = [round(float(x) * 10) / 10 for x in bbox]\n",
        "\n",
        "        self.bbox_data.append({'image_id': int(image_id),\n",
        "                               'category_id': self.coco_cats[int(category_id)],\n",
        "                               'bbox': bbox,\n",
        "                               'score': float(score)})\n",
        "\n",
        "    def add_mask(self, image_id: int, category_id: int, segmentation: np.ndarray, score: float):\n",
        "        \"\"\" The segmentation should be the full mask, the size of the image and with size [h, w]. \"\"\"\n",
        "        rle = pycocotools.mask.encode(np.asfortranarray(segmentation.astype(np.uint8)))\n",
        "        rle['counts'] = rle['counts'].decode('ascii')  # json.dump doesn't like bytes strings\n",
        "\n",
        "        self.mask_data.append({'image_id': int(image_id),\n",
        "                               'category_id': self.coco_cats[int(category_id)],\n",
        "                               'segmentation': rle,\n",
        "                               'score': float(score)})\n",
        "\n",
        "    def dump(self):\n",
        "        dump_arguments = [(self.bbox_data, f'/content/drive/MyDrive/yolact/results/bbox_detections.json'),\n",
        "                          (self.mask_data, f'/content/drive/MyDrive/yolact/results/mask_detections.json')]\n",
        "\n",
        "        for data, path in dump_arguments:\n",
        "            with open(path, 'w') as f:\n",
        "                json.dump(data, f)\n",
        "\n",
        "\n",
        "class APDataObject:\n",
        "    \"\"\"Stores all the information necessary to calculate the AP for one IoU and one class.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data_points = []\n",
        "        self.num_gt_positives = 0\n",
        "\n",
        "    def push(self, score: float, is_true: bool):\n",
        "        self.data_points.append((score, is_true))\n",
        "\n",
        "    def add_gt_positives(self, num_positives: int):\n",
        "        self.num_gt_positives += num_positives\n",
        "\n",
        "    def is_empty(self) -> bool:\n",
        "        return len(self.data_points) == 0 and self.num_gt_positives == 0\n",
        "\n",
        "    def get_ap(self) -> float:\n",
        "        if self.num_gt_positives == 0:\n",
        "            return 0\n",
        "\n",
        "        # Sort descending by score\n",
        "        self.data_points.sort(key=lambda x: -x[0])\n",
        "\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "        num_true = 0\n",
        "        num_false = 0\n",
        "\n",
        "        # Compute the precision-recall curve. The x axis is recalls and the y axis precisions.\n",
        "        for datum in self.data_points:\n",
        "            # datum[1] is whether the detection a true or false positive\n",
        "            if datum[1]:\n",
        "                num_true += 1\n",
        "            else:\n",
        "                num_false += 1\n",
        "\n",
        "            precision = num_true / (num_true + num_false)\n",
        "            recall = num_true / self.num_gt_positives\n",
        "\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "\n",
        "        # Smooth the curve by computing [max(precisions[i:]) for i in range(len(precisions))]\n",
        "        # Basically, remove any temporary dips from the curve.\n",
        "        # At least that's what I think, idk. COCOEval did it so I do too.\n",
        "        for i in range(len(precisions) - 1, 0, -1):\n",
        "            if precisions[i] > precisions[i - 1]:\n",
        "                precisions[i - 1] = precisions[i]\n",
        "\n",
        "        # Compute the integral of precision(recall) d_recall from recall=0->1 using fixed-length riemann summation with 101 bars.\n",
        "        y_range = [0] * 101  # idx 0 is recall == 0.0 and idx 100 is recall == 1.00\n",
        "        x_range = np.array([x / 100 for x in range(101)])\n",
        "        recalls = np.array(recalls)\n",
        "\n",
        "        # I realize this is weird, but all it does is find the nearest precision(x) for a given x in x_range.\n",
        "        # Basically, if the closest recall we have to 0.01 is 0.009 this sets precision(0.01) = precision(0.009).\n",
        "        # I approximate the integral this way, because that's how COCOEval does it.\n",
        "        indices = np.searchsorted(recalls, x_range, side='left')\n",
        "        for bar_idx, precision_idx in enumerate(indices):\n",
        "            if precision_idx < len(precisions):\n",
        "                y_range[bar_idx] = precisions[precision_idx]\n",
        "\n",
        "        # Finally compute the riemann sum to get our integral.\n",
        "        # avg([precision(x) for x in 0:0.01:1])\n",
        "        return sum(y_range) / len(y_range)\n",
        "\n",
        "\n",
        "def prep_metrics(ap_data, ids_p, classes_p, boxes_p, masks_p, gt, gt_masks, height, width, iou_thres):\n",
        "    gt_boxes = gt[:, :4]\n",
        "    gt_boxes[:, [0, 2]] *= width\n",
        "    gt_boxes[:, [1, 3]] *= height\n",
        "    gt_classes = gt[:, 4].int().tolist()\n",
        "    gt_masks = gt_masks.reshape(-1, height * width)\n",
        "    masks_p = masks_p.reshape(-1, height * width)\n",
        "\n",
        "    mask_iou_cache = mask_iou(masks_p, gt_masks)\n",
        "    bbox_iou_cache = box_iou(boxes_p.float(), gt_boxes.float()).cpu()\n",
        "\n",
        "    for _class in set(ids_p + gt_classes):\n",
        "        num_gt_per_class = gt_classes.count(_class)\n",
        "\n",
        "        for iouIdx in range(len(iou_thres)):\n",
        "            iou_threshold = iou_thres[iouIdx]\n",
        "\n",
        "            for iou_type, iou_func in zip(['box', 'mask'], [bbox_iou_cache, mask_iou_cache]):\n",
        "                gt_used = [False] * len(gt_classes)\n",
        "                ap_obj = ap_data[iou_type][iouIdx][_class]\n",
        "                ap_obj.add_gt_positives(num_gt_per_class)\n",
        "\n",
        "                for i, pred_class in enumerate(ids_p):\n",
        "                    if pred_class != _class:\n",
        "                        continue\n",
        "\n",
        "                    max_iou_found = iou_threshold\n",
        "                    max_match_idx = -1\n",
        "                    for j, gt_class in enumerate(gt_classes):\n",
        "                        if gt_used[j] or gt_class != _class:\n",
        "                            continue\n",
        "\n",
        "                        iou = iou_func[i, j].item()\n",
        "\n",
        "                        if iou > max_iou_found:\n",
        "                            max_iou_found = iou\n",
        "                            max_match_idx = j\n",
        "\n",
        "                    if max_match_idx >= 0:\n",
        "                        gt_used[max_match_idx] = True\n",
        "                        ap_obj.push(classes_p[i], True)\n",
        "                    else:\n",
        "                        ap_obj.push(classes_p[i], False)\n",
        "\n",
        "\n",
        "def calc_map(ap_data, iou_thres, num_classes, step):\n",
        "    print('\\nCalculating mAP...')\n",
        "    aps = [{'box': [], 'mask': []} for _ in iou_thres]\n",
        "\n",
        "    for _class in range(num_classes):\n",
        "        for iou_idx in range(len(iou_thres)):\n",
        "            for iou_type in ('box', 'mask'):\n",
        "                ap_obj = ap_data[iou_type][iou_idx][_class]\n",
        "\n",
        "                if not ap_obj.is_empty():\n",
        "                    aps[iou_idx][iou_type].append(ap_obj.get_ap())\n",
        "\n",
        "    all_maps = {'box': OrderedDict(), 'mask': OrderedDict()}\n",
        "\n",
        "    for iou_type in ('box', 'mask'):\n",
        "        all_maps[iou_type]['all'] = 0  # Make this first in the ordereddict\n",
        "\n",
        "        for i, threshold in enumerate(iou_thres):\n",
        "            mAP = sum(aps[i][iou_type]) / len(aps[i][iou_type]) * 100 if len(aps[i][iou_type]) > 0 else 0\n",
        "            all_maps[iou_type][int(threshold * 100)] = mAP\n",
        "\n",
        "        all_maps[iou_type]['all'] = (sum(all_maps[iou_type].values()) / (len(all_maps[iou_type].values()) - 1))\n",
        "\n",
        "    row1 = list(all_maps['box'].keys())\n",
        "    row1.insert(0, f'{step // 1000}k' if step else '')\n",
        "\n",
        "    row2 = list(all_maps['box'].values())\n",
        "    row2 = [round(aa, 2) for aa in row2]\n",
        "    row2.insert(0, 'box')\n",
        "\n",
        "    row3 = list(all_maps['mask'].values())\n",
        "    row3 = [round(aa, 2) for aa in row3]\n",
        "    row3.insert(0, 'mask')\n",
        "\n",
        "    table = [row1, row2, row3]\n",
        "    table = AsciiTable(table)\n",
        "    return table.table, row2, row3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "due0fk5H6B8n",
        "outputId": "e73b86ed-a3bf-4ffe-f06f-4713960f64d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: terminaltables\n",
            "Successfully installed terminaltables-3.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch.fft as fft\n",
        "import pdb\n",
        "\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "class PredictionModule(nn.Module):\n",
        "    def __init__(self, cfg, coef_dim=32):\n",
        "        super().__init__()\n",
        "        self.num_classes = cfg.num_classes\n",
        "        self.coef_dim = coef_dim\n",
        "\n",
        "        self.upfeature = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "                                       nn.ReLU(inplace=True))\n",
        "        self.bbox_layer = nn.Conv2d(256, len(cfg.aspect_ratios) * 4, kernel_size=3, padding=1)\n",
        "        self.conf_layer = nn.Conv2d(256, len(cfg.aspect_ratios) * self.num_classes, kernel_size=3, padding=1)\n",
        "        self.coef_layer = nn.Sequential(nn.Conv2d(256, len(cfg.aspect_ratios) * self.coef_dim,\n",
        "                                                  kernel_size=3, padding=1),\n",
        "                                        nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upfeature(x)\n",
        "        conf = self.conf_layer(x).permute(0, 2, 3, 1).reshape(x.size(0), -1, self.num_classes)\n",
        "        box = self.bbox_layer(x).permute(0, 2, 3, 1).reshape(x.size(0), -1, 4)\n",
        "        coef = self.coef_layer(x).permute(0, 2, 3, 1).reshape(x.size(0), -1, self.coef_dim)\n",
        "        return conf, box, coef\n",
        "\n",
        "\n",
        "class ProtoNet(nn.Module):\n",
        "    def __init__(self, coef_dim):\n",
        "        super().__init__()\n",
        "        self.proto1 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(inplace=True))\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.proto2 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(256, coef_dim, kernel_size=1, stride=1),\n",
        "                                    nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proto1(x)\n",
        "        x = self.upsample(x)\n",
        "        x = self.proto2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DFFN(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.p3_p4_p5_conv = nn.ModuleList([nn.Conv2d(x, 256, kernel_size=1) for x in self.in_channels])\n",
        "\n",
        "        self.n3_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.n4_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.n5_conv = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.m3_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.m4_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        # Note the below line compare WHEN TRAINING!!! according to the minimal implementation ReLU is applied\n",
        "        # self.n6_down = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1), nn.ReLU(inplace=True))\n",
        "        self.n6_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.n7_down = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, backbone_outs):\n",
        "        # Calculate P5 from c5 in this implementation backbone_outs[2]==c5 crop\n",
        "        p5 = self.p3_p4_p5_conv[2](backbone_outs[2])\n",
        "\n",
        "        # Calculate P4 c4==backbone_outs[1]\n",
        "        p4_upsample = F.interpolate(p5, size=(34,34), mode='bilinear', align_corners=False)\n",
        "        p4_conv = self.p3_p4_p5_conv[1](backbone_outs[1])\n",
        "        p4 = p4_upsample + p4_conv\n",
        "\n",
        "        # Calculate P3 c3==backbone_outs[0], c2 is already removed\n",
        "        p3_upsample = F.interpolate(p4, size=(68,68), mode='bilinear', align_corners=False)\n",
        "        p3_conv = self.p3_p4_p5_conv[0](backbone_outs[0])\n",
        "        p3 = p3_upsample + p3_conv\n",
        "\n",
        "        # Calculate N3\n",
        "        n3 = self.n3_conv(p3)\n",
        "\n",
        "        # Calculate M3 and N4\n",
        "        m3 = self.m3_down(n3)\n",
        "        n4 = self.n4_conv(p4 + m3)\n",
        "\n",
        "        # Calculate M4 and N5\n",
        "        m4 = self.m4_down(n4)\n",
        "        n5 = self.n5_conv(p5 + m4)\n",
        "\n",
        "        # Calculate N6 and N7\n",
        "        n6 = self.n6_down(n5)\n",
        "        n7 = self.n7_down(n6)\n",
        "\n",
        "        dffn_outs = []\n",
        "        dffn_outs = n3, n4, n5, n6, n7\n",
        "\n",
        "        return dffn_outs\n",
        "\n",
        "\n",
        "class DualDomainAttentionModule(nn.Module):\n",
        "    def __init__(self, C = 256, reduction=16):\n",
        "        # super(DualDomainAttentionModule, self).__init__()\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        self.fc1 = nn.Linear(C, C // reduction)\n",
        "        self.fc2 = nn.Linear(C // reduction, C)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        dct = fft.rfft2(x, dim=(-2, -1))\n",
        "        dct = dct[..., :2, :2]  # Keep only the low-frequency components\n",
        "        dct = torch.norm(dct, dim=(-2, -1))  # Compute the magnitude of the complex numbers\n",
        "        dct = dct.view(dct.size(0), -1)  # Flatten the features\n",
        "\n",
        "        y0 = F.relu(self.fc1(dct))\n",
        "        y1 = self.fc2(y0)\n",
        "        weights = self.sigmoid(y1)\n",
        "\n",
        "        out = x * weights.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dffn = DFFN(in_channels)\n",
        "        self.ddam = DualDomainAttentionModule()\n",
        "\n",
        "    def forward(self, backbone_outs):\n",
        "        dffn_outs = self.dffn(backbone_outs)\n",
        "\n",
        "        ddam_outs = []\n",
        "        for dffn_out in dffn_outs:\n",
        "            ddam_out = self.ddam(dffn_out)\n",
        "            ddam_outs.append(ddam_out)\n",
        "\n",
        "        p3, p4, p5, p6, p7 = ddam_outs\n",
        "        return p3, p4, p5, p6, p7\n",
        "\n",
        "\n",
        "\n",
        "class Yolact(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.coef_dim = 32\n",
        "\n",
        "        self.backbone = ResNet(layers=(3, 4, 23, 3))\n",
        "        self.fpn = FPN(in_channels=(512, 1024, 2048))\n",
        "\n",
        "        self.proto_net = ProtoNet(coef_dim=self.coef_dim)\n",
        "        self.prediction_layers = PredictionModule(cfg, coef_dim=self.coef_dim)\n",
        "\n",
        "        self.anchors = []\n",
        "        fpn_fm_shape = [math.ceil(cfg.img_size / stride) for stride in (8, 16, 32, 64, 128)]\n",
        "        for i, size in enumerate(fpn_fm_shape):\n",
        "            self.anchors += make_anchors(self.cfg, size, size, self.cfg.scales[i])\n",
        "\n",
        "        if cfg.mode == 'train':\n",
        "            self.semantic_seg_conv = nn.Conv2d(256, cfg.num_classes - 1, kernel_size=1)\n",
        "\n",
        "        # init weights, backbone weights will be covered later\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(module.weight.data)\n",
        "\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "    def load_weights(self, weight, cuda):\n",
        "        if cuda:\n",
        "            state_dict = torch.load(weight)\n",
        "        else:\n",
        "            state_dict = torch.load(weight, map_location='cpu')\n",
        "\n",
        "        for key in list(state_dict.keys()):\n",
        "            if self.cfg.mode != 'train' and key.startswith('semantic_seg_conv'):\n",
        "                del state_dict[key]\n",
        "\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "        print(f'Model loaded with {weight}.\\n')\n",
        "        print(f'Number of all parameters: {sum([p.numel() for p in self.parameters()])}\\n')\n",
        "\n",
        "    def init_weights(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                init.constant_(param, 0.0)\n",
        "            elif 'weight' in name:\n",
        "                if '.conv.' in name:\n",
        "                    init.xavier_normal_(param)\n",
        "                elif '.bn.' in name:\n",
        "                    init.constant_(param, 1.0)\n",
        "                elif '.bias.' in name:\n",
        "                    init.constant_(param, 0.0)\n",
        "\n",
        "        print(f'Model initialized.\\n')\n",
        "        # print(f'Number of all parameters: {sum([p.numel() for p in self.parameters()])}\\n')\n",
        "\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, img, box_classes=None, masks_gt=None):\n",
        "\n",
        "        outs = self.backbone(img)\n",
        "\n",
        "        outs = self.fpn(outs[1:4])\n",
        "\n",
        "        proto_out = self.proto_net(outs[0])  # feature map P3\n",
        "        proto_out = proto_out.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        class_pred, box_pred, coef_pred = [], [], []\n",
        "\n",
        "        for aa in outs:\n",
        "            class_p, box_p, coef_p = self.prediction_layers(aa)\n",
        "            class_pred.append(class_p)\n",
        "            box_pred.append(box_p)\n",
        "            coef_pred.append(coef_p)\n",
        "\n",
        "        class_pred = torch.cat(class_pred, dim=1)\n",
        "        box_pred = torch.cat(box_pred, dim=1)\n",
        "        coef_pred = torch.cat(coef_pred, dim=1)\n",
        "\n",
        "        if self.training:\n",
        "            seg_pred = self.semantic_seg_conv(outs[0])\n",
        "            return self.compute_loss(class_pred, box_pred, coef_pred, proto_out, seg_pred, box_classes, masks_gt)\n",
        "        else:\n",
        "            class_pred = F.softmax(class_pred, -1)\n",
        "            return class_pred, box_pred, coef_pred, proto_out\n",
        "\n",
        "    def compute_loss(self, class_p, box_p, coef_p, proto_p, seg_p, box_class, mask_gt):\n",
        "        device = class_p.device\n",
        "        class_gt = [None] * len(box_class)\n",
        "        batch_size = box_p.size(0)\n",
        "\n",
        "        if isinstance(self.anchors, list):\n",
        "            self.anchors = torch.tensor(self.anchors, device=device).reshape(-1, 4)\n",
        "\n",
        "        num_anchors = self.anchors.shape[0]\n",
        "\n",
        "        all_offsets = torch.zeros((batch_size, num_anchors, 4), dtype=torch.float32, device=device)\n",
        "        conf_gt = torch.zeros((batch_size, num_anchors), dtype=torch.int64, device=device)\n",
        "        anchor_max_gt = torch.zeros((batch_size, num_anchors, 4), dtype=torch.float32, device=device)\n",
        "        anchor_max_i = torch.zeros((batch_size, num_anchors), dtype=torch.int64, device=device)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            box_gt = box_class[i][:, :-1]\n",
        "            class_gt[i] = box_class[i][:, -1].long()\n",
        "\n",
        "            all_offsets[i], conf_gt[i], anchor_max_gt[i], anchor_max_i[i] = match(self.cfg, box_gt,\n",
        "                                                                                  self.anchors, class_gt[i])\n",
        "\n",
        "        assert (not all_offsets.requires_grad) and (not conf_gt.requires_grad) and \\\n",
        "               (not anchor_max_i.requires_grad), 'Incorrect computation graph, check the grad.'\n",
        "\n",
        "        pos_bool = conf_gt > 0\n",
        "\n",
        "        loss_c = self.category_loss(class_p, conf_gt, pos_bool)\n",
        "        loss_b = self.box_loss(box_p, all_offsets, pos_bool)\n",
        "        loss_m = self.lincomb_mask_loss(pos_bool, anchor_max_i, coef_p, proto_p, mask_gt, anchor_max_gt)\n",
        "        loss_s = self.semantic_seg_loss(seg_p, mask_gt, class_gt)\n",
        "        return loss_c, loss_b, loss_m, loss_s\n",
        "\n",
        "    def category_loss(self, class_p, conf_gt, pos_bool, np_ratio=3):\n",
        "        batch_conf = class_p.reshape(-1, self.cfg.num_classes)\n",
        "\n",
        "        batch_conf_max = batch_conf.max()\n",
        "        mark = torch.log(torch.sum(torch.exp(batch_conf - batch_conf_max), 1)) + batch_conf_max - batch_conf[:, 0]\n",
        "\n",
        "        mark = mark.reshape(class_p.size(0), -1)\n",
        "        mark[pos_bool] = 0  # filter out pos boxes\n",
        "        mark[conf_gt < 0] = 0  # filter out neutrals (conf_gt = -1)\n",
        "\n",
        "        _, idx = mark.sort(1, descending=True)\n",
        "        _, idx_rank = idx.sort(1)\n",
        "\n",
        "        num_pos = pos_bool.long().sum(1, keepdim=True)\n",
        "        num_neg = torch.clamp(np_ratio * num_pos, max=pos_bool.size(1) - 1)\n",
        "        neg_bool = idx_rank < num_neg.expand_as(idx_rank)\n",
        "\n",
        "        neg_bool[pos_bool] = 0\n",
        "        neg_bool[conf_gt < 0] = 0  # Filter out neutrals\n",
        "\n",
        "        class_p_mined = class_p[(pos_bool + neg_bool)].reshape(-1, self.cfg.num_classes)\n",
        "        class_gt_mined = conf_gt[(pos_bool + neg_bool)]\n",
        "\n",
        "        return self.cfg.conf_alpha * F.cross_entropy(class_p_mined, class_gt_mined, reduction='sum') / num_pos.sum()\n",
        "\n",
        "    def box_loss(self, box_p, all_offsets, pos_bool):\n",
        "        num_pos = pos_bool.sum()\n",
        "        pos_box_p = box_p[pos_bool, :]\n",
        "        pos_offsets = all_offsets[pos_bool, :]\n",
        "\n",
        "        return self.cfg.bbox_alpha * F.smooth_l1_loss(pos_box_p, pos_offsets, reduction='sum') / num_pos\n",
        "\n",
        "    def lincomb_mask_loss(self, pos_bool, anchor_max_i, coef_p, proto_p, mask_gt, anchor_max_gt):\n",
        "        proto_h, proto_w = proto_p.shape[1:3]\n",
        "        total_pos_num = pos_bool.sum()\n",
        "        loss_m = 0\n",
        "        for i in range(coef_p.size(0)):\n",
        "            # downsample the gt mask to the size of 'proto_p'\n",
        "            downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (proto_h, proto_w), mode='bilinear',\n",
        "                                              align_corners=False).squeeze(0)\n",
        "            downsampled_masks = downsampled_masks.permute(1, 2, 0).contiguous()\n",
        "            # binarize the gt mask because of the downsample operation\n",
        "            downsampled_masks = downsampled_masks.gt(0.5).float()\n",
        "\n",
        "            pos_anchor_i = anchor_max_i[i][pos_bool[i]]\n",
        "            pos_anchor_box = anchor_max_gt[i][pos_bool[i]]\n",
        "            pos_coef = coef_p[i][pos_bool[i]]\n",
        "\n",
        "            if pos_anchor_i.size(0) == 0:\n",
        "                continue\n",
        "\n",
        "            # If exceeds the number of masks for training, select a random subset\n",
        "            old_num_pos = pos_coef.size(0)\n",
        "            if old_num_pos > self.cfg.masks_to_train:\n",
        "                perm = torch.randperm(pos_coef.size(0))\n",
        "                select = perm[:self.cfg.masks_to_train]\n",
        "                pos_coef = pos_coef[select]\n",
        "                pos_anchor_i = pos_anchor_i[select]\n",
        "                pos_anchor_box = pos_anchor_box[select]\n",
        "\n",
        "            num_pos = pos_coef.size(0)\n",
        "\n",
        "            pos_mask_gt = downsampled_masks[:, :, pos_anchor_i]\n",
        "\n",
        "            # @ means dot product\n",
        "            mask_p = torch.sigmoid(proto_p[i] @ pos_coef.t())\n",
        "            mask_p = crop(mask_p, pos_anchor_box)  # pos_anchor_box.shape: (num_pos, 4)\n",
        "            # TODO: grad out of gt box is 0, should it be modified?\n",
        "            # TODO: need an upsample before computing loss?\n",
        "            mask_loss = F.binary_cross_entropy(torch.clamp(mask_p, 0, 1), pos_mask_gt, reduction='none')\n",
        "\n",
        "            anchor_area = (pos_anchor_box[:, 2] - pos_anchor_box[:, 0]) * (pos_anchor_box[:, 3] - pos_anchor_box[:, 1])\n",
        "            mask_loss = mask_loss.sum(dim=(0, 1)) / anchor_area\n",
        "\n",
        "            if old_num_pos > num_pos:\n",
        "                mask_loss *= old_num_pos / num_pos\n",
        "\n",
        "            loss_m += torch.sum(mask_loss)\n",
        "\n",
        "        return self.cfg.mask_alpha * loss_m / proto_h / proto_w / total_pos_num\n",
        "\n",
        "    def semantic_seg_loss(self, segmentation_p, mask_gt, class_gt):\n",
        "        # Note classes here exclude the background class, so num_classes = cfg.num_classes - 1\n",
        "        batch_size, num_classes, mask_h, mask_w = segmentation_p.size()\n",
        "        loss_s = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            cur_segment = segmentation_p[i]\n",
        "            cur_class_gt = class_gt[i]\n",
        "\n",
        "            downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (mask_h, mask_w), mode='bilinear',\n",
        "                                              align_corners=False).squeeze(0)\n",
        "            downsampled_masks = downsampled_masks.gt(0.5).float()\n",
        "\n",
        "            # Construct Semantic Segmentation\n",
        "            segment_gt = torch.zeros_like(cur_segment, requires_grad=False)\n",
        "            for j in range(downsampled_masks.size(0)):\n",
        "                segment_gt[cur_class_gt[j]] = torch.max(segment_gt[cur_class_gt[j]], downsampled_masks[j])\n",
        "\n",
        "            loss_s += F.binary_cross_entropy_with_logits(cur_segment, segment_gt, reduction='sum')\n",
        "\n",
        "        return self.cfg.semantic_alpha * loss_s / mask_h / mask_w / batch_size"
      ],
      "metadata": {
        "id": "-C8uqPY57FKG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "# from cython_nms import nms as cnms\n",
        "import pdb\n",
        "\n",
        "\n",
        "def fast_nms(box_thre, coef_thre, class_thre, cfg):\n",
        "    class_thre, idx = class_thre.sort(1, descending=True)  # [80, 64 (the number of kept boxes)]\n",
        "\n",
        "    idx = idx[:, :cfg.top_k]\n",
        "    class_thre = class_thre[:, :cfg.top_k]\n",
        "\n",
        "    num_classes, num_dets = idx.size()\n",
        "    box_thre = box_thre[idx.reshape(-1), :].reshape(num_classes, num_dets, 4)  # [80, 64, 4]\n",
        "    coef_thre = coef_thre[idx.reshape(-1), :].reshape(num_classes, num_dets, -1)  # [80, 64, 32]\n",
        "\n",
        "    iou = box_iou(box_thre, box_thre)\n",
        "    iou.triu_(diagonal=1)\n",
        "    iou_max, _ = iou.max(dim=1)\n",
        "\n",
        "    # Now just filter out the ones higher than the threshold\n",
        "    keep = (iou_max <= cfg.nms_iou_thre)\n",
        "\n",
        "    # Assign each kept detection to its corresponding class\n",
        "    class_ids = torch.arange(num_classes, device=box_thre.device)[:, None].expand_as(keep)\n",
        "\n",
        "    class_ids, box_nms, coef_nms, class_nms = class_ids[keep], box_thre[keep], coef_thre[keep], class_thre[keep]\n",
        "\n",
        "    # Only keep the top cfg.max_num_detections highest scores across all classes\n",
        "    class_nms, idx = class_nms.sort(0, descending=True)\n",
        "\n",
        "    idx = idx[:cfg.max_detections]\n",
        "    class_nms = class_nms[:cfg.max_detections]\n",
        "\n",
        "    class_ids = class_ids[idx]\n",
        "    box_nms = box_nms[idx]\n",
        "    coef_nms = coef_nms[idx]\n",
        "\n",
        "    return box_nms, coef_nms, class_ids, class_nms\n",
        "\n",
        "\n",
        "def fast_nms_numpy(box_thre, coef_thre, class_thre, cfg):\n",
        "    # descending sort\n",
        "    idx = np.argsort(-class_thre, axis=1)\n",
        "    class_thre = np.sort(class_thre, axis=1)[:, ::-1]\n",
        "\n",
        "    idx = idx[:, :cfg.top_k]\n",
        "    class_thre = class_thre[:, :cfg.top_k]\n",
        "\n",
        "    num_classes, num_dets = idx.shape\n",
        "    box_thre = box_thre[idx.reshape(-1), :].reshape(num_classes, num_dets, 4)  # [80, 64, 4]\n",
        "    coef_thre = coef_thre[idx.reshape(-1), :].reshape(num_classes, num_dets, -1)  # [80, 64, 32]\n",
        "\n",
        "    iou = box_iou_numpy(box_thre, box_thre)\n",
        "    iou = np.triu(iou, k=1)\n",
        "    iou_max = np.max(iou, axis=1)\n",
        "\n",
        "    # Now just filter out the ones higher than the threshold\n",
        "    keep = (iou_max <= cfg.nms_iou_thre)\n",
        "\n",
        "    # Assign each kept detection to its corresponding class\n",
        "    class_ids = np.tile(np.arange(num_classes)[:, None], (1, keep.shape[1]))\n",
        "\n",
        "    class_ids, box_nms, coef_nms, class_nms = class_ids[keep], box_thre[keep], coef_thre[keep], class_thre[keep]\n",
        "\n",
        "    # Only keep the top cfg.max_num_detections highest scores across all classes\n",
        "    idx = np.argsort(-class_nms, axis=0)\n",
        "    class_nms = np.sort(class_nms, axis=0)[::-1]\n",
        "\n",
        "    idx = idx[:cfg.max_detections]\n",
        "    class_nms = class_nms[:cfg.max_detections]\n",
        "\n",
        "    class_ids = class_ids[idx]\n",
        "    box_nms = box_nms[idx]\n",
        "    coef_nms = coef_nms[idx]\n",
        "\n",
        "    return box_nms, coef_nms, class_ids, class_nms\n",
        "\n",
        "\n",
        "def traditional_nms(boxes, masks, scores, cfg):\n",
        "    num_classes = scores.size(0)\n",
        "\n",
        "    idx_lst, cls_lst, scr_lst = [], [], []\n",
        "\n",
        "    # Multiplying by max_size is necessary because of how cnms computes its area and intersections\n",
        "    boxes = boxes * cfg.img_size\n",
        "\n",
        "    for _cls in range(num_classes):\n",
        "        cls_scores = scores[_cls, :]\n",
        "        conf_mask = cls_scores > cfg.nms_score_thre\n",
        "        idx = torch.arange(cls_scores.size(0), device=boxes.device)\n",
        "\n",
        "        cls_scores = cls_scores[conf_mask]\n",
        "        idx = idx[conf_mask]\n",
        "\n",
        "        if cls_scores.size(0) == 0:\n",
        "            continue\n",
        "\n",
        "        preds = torch.cat([boxes[conf_mask], cls_scores[:, None]], dim=1).cpu().numpy()\n",
        "        keep = cnms(preds, cfg.nms_iou_thre)\n",
        "        keep = torch.tensor(keep, device=boxes.device).long()\n",
        "\n",
        "        idx_lst.append(idx[keep])\n",
        "        cls_lst.append(keep * 0 + _cls)\n",
        "        scr_lst.append(cls_scores[keep])\n",
        "\n",
        "    idx = torch.cat(idx_lst, dim=0)\n",
        "    class_ids = torch.cat(cls_lst, dim=0)\n",
        "    scores = torch.cat(scr_lst, dim=0)\n",
        "\n",
        "    scores, idx2 = scores.sort(0, descending=True)\n",
        "    idx2 = idx2[:cfg.max_detections]\n",
        "    scores = scores[:cfg.max_detections]\n",
        "\n",
        "    idx = idx[idx2]\n",
        "    class_ids = class_ids[idx2]\n",
        "\n",
        "    # Undo the multiplication above\n",
        "    return boxes[idx] / cfg.img_size, masks[idx], class_ids, scores\n",
        "\n",
        "\n",
        "def nms(class_pred, box_pred, coef_pred, proto_out, anchors, cfg):\n",
        "    class_p = class_pred.squeeze()  # [19248, 81]\n",
        "    box_p = box_pred.squeeze()  # [19248, 4]\n",
        "    coef_p = coef_pred.squeeze()  # [19248, 32]\n",
        "    proto_p = proto_out.squeeze()  # [138, 138, 32]\n",
        "\n",
        "    if isinstance(anchors, list):\n",
        "        anchors = torch.tensor(anchors, device=class_p.device).reshape(-1, 4)\n",
        "\n",
        "    class_p = class_p.transpose(1, 0).contiguous()  # [81, 19248]\n",
        "\n",
        "    # exclude the background class\n",
        "    class_p = class_p[1:, :]\n",
        "    # get the max score class of 19248 predicted boxes\n",
        "    class_p_max, _ = torch.max(class_p, dim=0)  # [19248]\n",
        "\n",
        "    # filter predicted boxes according the class score\n",
        "    keep = (class_p_max > cfg.nms_score_thre)\n",
        "    class_thre = class_p[:, keep]\n",
        "    box_thre, anchor_thre, coef_thre = box_p[keep, :], anchors[keep, :], coef_p[keep, :]\n",
        "\n",
        "    # decode boxes\n",
        "    box_thre = torch.cat((anchor_thre[:, :2] + box_thre[:, :2] * 0.1 * anchor_thre[:, 2:],\n",
        "                          anchor_thre[:, 2:] * torch.exp(box_thre[:, 2:] * 0.2)), 1)\n",
        "    box_thre[:, :2] -= box_thre[:, 2:] / 2\n",
        "    box_thre[:, 2:] += box_thre[:, :2]\n",
        "\n",
        "    box_thre = torch.clip(box_thre, min=0., max=1.)\n",
        "\n",
        "    if class_thre.shape[1] == 0:\n",
        "        return None, None, None, None, None\n",
        "    else:\n",
        "        if not cfg.traditional_nms:\n",
        "            box_thre, coef_thre, class_ids, class_thre = fast_nms(box_thre, coef_thre, class_thre, cfg)\n",
        "        else:\n",
        "            box_thre, coef_thre, class_ids, class_thre = traditional_nms(box_thre, coef_thre, class_thre, cfg)\n",
        "\n",
        "        return class_ids, class_thre, box_thre, coef_thre, proto_p\n",
        "\n",
        "\n",
        "def nms_numpy(class_pred, box_pred, coef_pred, proto_out, anchors, cfg):\n",
        "    class_p = class_pred.squeeze()  # [19248, 81]\n",
        "    box_p = box_pred.squeeze()  # [19248, 4]\n",
        "    coef_p = coef_pred.squeeze()  # [19248, 32]\n",
        "    proto_p = proto_out.squeeze()  # [138, 138, 32]\n",
        "    anchors = np.array(anchors).reshape(-1, 4)\n",
        "\n",
        "    class_p = class_p.transpose(1, 0)\n",
        "    # exclude the background class\n",
        "    class_p = class_p[1:, :]\n",
        "    # get the max score class of 19248 predicted boxes\n",
        "\n",
        "    class_p_max = np.max(class_p, axis=0)  # [19248]\n",
        "\n",
        "    # filter predicted boxes according the class score\n",
        "    keep = (class_p_max > cfg.nms_score_thre)\n",
        "    class_thre = class_p[:, keep]\n",
        "\n",
        "    box_thre, anchor_thre, coef_thre = box_p[keep, :], anchors[keep, :], coef_p[keep, :]\n",
        "\n",
        "    # decode boxes\n",
        "    box_thre = np.concatenate((anchor_thre[:, :2] + box_thre[:, :2] * 0.1 * anchor_thre[:, 2:],\n",
        "                               anchor_thre[:, 2:] * np.exp(box_thre[:, 2:] * 0.2)), axis=1)\n",
        "    box_thre[:, :2] -= box_thre[:, 2:] / 2\n",
        "    box_thre[:, 2:] += box_thre[:, :2]\n",
        "\n",
        "    if class_thre.shape[1] == 0:\n",
        "        return None, None, None, None, None\n",
        "    else:\n",
        "        assert not cfg.traditional_nms, 'Traditional nms is not supported with numpy.'\n",
        "        box_thre, coef_thre, class_ids, class_thre = fast_nms_numpy(box_thre, coef_thre, class_thre, cfg)\n",
        "        return class_ids, class_thre, box_thre, coef_thre, proto_p\n",
        "\n",
        "\n",
        "def after_nms(ids_p, class_p, box_p, coef_p, proto_p, img_h, img_w, cfg=None, img_name=None):\n",
        "    if ids_p is None:\n",
        "        return None, None, None, None\n",
        "\n",
        "    if cfg and cfg.visual_thre > 0:\n",
        "        keep = class_p >= cfg.visual_thre\n",
        "        if not keep.any():\n",
        "            return None, None, None, None\n",
        "\n",
        "        ids_p = ids_p[keep]\n",
        "        class_p = class_p[keep]\n",
        "        box_p = box_p[keep]\n",
        "        coef_p = coef_p[keep]\n",
        "\n",
        "    if cfg and cfg.save_lincomb:\n",
        "        draw_lincomb(proto_p, coef_p, img_name)\n",
        "\n",
        "    masks = torch.sigmoid(torch.matmul(proto_p, coef_p.t()))\n",
        "\n",
        "    if not cfg or not cfg.no_crop:  # Crop masks by box_p\n",
        "        masks = crop_box(masks, box_p)\n",
        "\n",
        "    masks = masks.permute(2, 0, 1).contiguous()\n",
        "\n",
        "    ori_size = max(img_h, img_w)\n",
        "    # in OpenCV, cv2.resize is `align_corners=False`.\n",
        "    masks = F.interpolate(masks.unsqueeze(0), (ori_size, ori_size), mode='bilinear', align_corners=False).squeeze(0)\n",
        "    masks.gt_(0.5)  # Binarize the masks because of interpolation.\n",
        "    masks = masks[:, 0: img_h, :] if img_h < img_w else masks[:, :, 0: img_w]\n",
        "\n",
        "    box_p *= ori_size\n",
        "    box_p = box_p.int()\n",
        "\n",
        "    return ids_p, class_p, box_p, masks\n",
        "\n",
        "\n",
        "def after_nms_numpy(ids_p, class_p, box_p, coef_p, proto_p, img_h, img_w, cfg=None):\n",
        "    def np_sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    if ids_p is None:\n",
        "        return None, None, None, None\n",
        "\n",
        "    if cfg and cfg.visual_thre > 0:\n",
        "        keep = class_p >= cfg.visual_thre\n",
        "        if not keep.any():\n",
        "            return None, None, None, None\n",
        "\n",
        "        ids_p = ids_p[keep]\n",
        "        class_p = class_p[keep]\n",
        "        box_p = box_p[keep]\n",
        "        coef_p = coef_p[keep]\n",
        "\n",
        "    assert not cfg.save_lincomb, 'save_lincomb is not supported in onnx mode.'\n",
        "\n",
        "    masks = np_sigmoid(np.matmul(proto_p, coef_p.T))\n",
        "\n",
        "    if not cfg or not cfg.no_crop:  # Crop masks by box_p\n",
        "        masks = crop_numpy(masks, box_p)\n",
        "\n",
        "    ori_size = max(img_h, img_w)\n",
        "    masks = cv2.resize(masks, (ori_size, ori_size), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    if masks.ndim == 2:\n",
        "        masks = masks[:, :, None]\n",
        "\n",
        "    masks = np.transpose(masks, (2, 0, 1))\n",
        "    masks = masks > 0.5  # Binarize the masks because of interpolation.\n",
        "    masks = masks[:, 0: img_h, :] if img_h < img_w else masks[:, :, 0: img_w]\n",
        "\n",
        "    box_p *= ori_size\n",
        "    box_p = box_p.astype('int32')\n",
        "\n",
        "    return ids_p, class_p, box_p, masks\n",
        "\n",
        "\n",
        "def draw_lincomb(proto_data, masks, img_name):\n",
        "    for kdx in range(1):\n",
        "        jdx = kdx + 0\n",
        "        coeffs = masks[jdx, :].cpu().numpy()\n",
        "        idx = np.argsort(-np.abs(coeffs))\n",
        "\n",
        "        coeffs_sort = coeffs[idx]\n",
        "        arr_h, arr_w = (4, 8)\n",
        "        p_h, p_w, _ = proto_data.size()\n",
        "        arr_img = np.zeros([p_h * arr_h, p_w * arr_w])\n",
        "        arr_run = np.zeros([p_h * arr_h, p_w * arr_w])\n",
        "\n",
        "        for y in range(arr_h):\n",
        "            for x in range(arr_w):\n",
        "                i = arr_w * y + x\n",
        "\n",
        "                if i == 0:\n",
        "                    running_total = proto_data[:, :, idx[i]].cpu().numpy() * coeffs_sort[i]\n",
        "                else:\n",
        "                    running_total += proto_data[:, :, idx[i]].cpu().numpy() * coeffs_sort[i]\n",
        "\n",
        "                running_total_nonlin = (1 / (1 + np.exp(-running_total)))\n",
        "\n",
        "                arr_img[y * p_h:(y + 1) * p_h, x * p_w:(x + 1) * p_w] = (proto_data[:, :, idx[i]] / torch.max(\n",
        "                    proto_data[:, :, idx[i]])).cpu().numpy() * coeffs_sort[i]\n",
        "                arr_run[y * p_h:(y + 1) * p_h, x * p_w:(x + 1) * p_w] = (running_total_nonlin > 0.5).astype(np.float)\n",
        "\n",
        "        arr_img = ((arr_img + 1) * 127.5).astype('uint8')\n",
        "        arr_img = cv2.applyColorMap(arr_img, cv2.COLORMAP_WINTER)\n",
        "        cv2.imwrite(f'/content/drive/MyDrive/yolact/results/images/lincomb_{img_name}', arr_img)\n",
        "\n",
        "\n",
        "def draw_img(ids_p, class_p, box_p, mask_p, img_origin, cfg, img_name=None, fps=None):\n",
        "    if ids_p is None:\n",
        "        return img_origin\n",
        "\n",
        "    if isinstance(ids_p, torch.Tensor):\n",
        "        ids_p = ids_p.cpu().numpy()\n",
        "        class_p = class_p.cpu().numpy()\n",
        "        box_p = box_p.cpu().numpy()\n",
        "        mask_p = mask_p.cpu().numpy()\n",
        "\n",
        "    num_detected = ids_p.shape[0]\n",
        "\n",
        "    img_fused = img_origin\n",
        "    if not cfg.hide_mask:\n",
        "        masks_semantic = mask_p * (ids_p[:, None, None] + 1)  # expand ids_p' shape for broadcasting\n",
        "        # The color of the overlap area is different because of the '%' operation.\n",
        "        masks_semantic = masks_semantic.astype('int').sum(axis=0) % (cfg.num_classes - 1)\n",
        "        color_masks = COLORS[masks_semantic].astype('uint8')\n",
        "        img_fused = cv2.addWeighted(color_masks, 0.4, img_origin, 0.6, gamma=0)\n",
        "\n",
        "        if cfg.cutout:\n",
        "            total_obj = (masks_semantic != 0)[:, :, None].repeat(3, 2)\n",
        "            total_obj = total_obj * img_origin\n",
        "            new_mask = ((masks_semantic == 0) * 255)[:, :, None].repeat(3, 2)\n",
        "            img_matting = (total_obj + new_mask).astype('uint8')\n",
        "            cv2.imwrite(f'/content/drive/MyDrive/yolact/results/images/{img_name}_total_obj.jpg', img_matting)\n",
        "\n",
        "            for i in range(num_detected):\n",
        "                one_obj = (mask_p[i])[:, :, None].repeat(3, 2)\n",
        "                one_obj = one_obj * img_origin\n",
        "                new_mask = ((mask_p[i] == 0) * 255)[:, :, None].repeat(3, 2)\n",
        "                x1, y1, x2, y2 = box_p[i, :]\n",
        "                img_matting = (one_obj + new_mask)[y1:y2, x1:x2, :]\n",
        "                cv2.imwrite(f'/content/drive/MyDrive/yolact/results/images/{img_name}_{i}.jpg', img_matting)\n",
        "    scale = 0.6\n",
        "    thickness = 1\n",
        "    font = cv2.FONT_HERSHEY_DUPLEX\n",
        "\n",
        "    if not cfg.hide_bbox:\n",
        "        for i in reversed(range(num_detected)):\n",
        "            x1, y1, x2, y2 = box_p[i, :]\n",
        "\n",
        "            color = COLORS[ids_p[i] + 1].tolist()\n",
        "            cv2.rectangle(img_fused, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "            class_name = cfg.class_names[ids_p[i]]\n",
        "            text_str = f'{class_name}: {class_p[i]:.2f}' if not cfg.hide_score else class_name\n",
        "\n",
        "            text_w, text_h = cv2.getTextSize(text_str, font, scale, thickness)[0]\n",
        "            cv2.rectangle(img_fused, (x1, y1), (x1 + text_w, y1 + text_h + 5), color, -1)\n",
        "            cv2.putText(img_fused, text_str, (x1, y1 + 15), font, scale, (255, 255, 255), thickness, cv2.LINE_AA)\n",
        "\n",
        "    if cfg.real_time:\n",
        "        fps_str = f'fps: {fps:.2f}'\n",
        "        text_w, text_h = cv2.getTextSize(fps_str, font, scale, thickness)[0]\n",
        "        # Create a shadow to show the fps more clearly\n",
        "        img_fused = img_fused.astype(np.float32)\n",
        "        img_fused[0:text_h + 8, 0:text_w + 8] *= 0.6\n",
        "        img_fused = img_fused.astype(np.uint8)\n",
        "        cv2.putText(img_fused, fps_str, (0, text_h + 2), font, scale, (255, 255, 255), thickness, cv2.LINE_AA)\n",
        "\n",
        "    return img_fused\n"
      ],
      "metadata": {
        "id": "zOioILc66hJc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "times = {}\n",
        "times.setdefault('batch', [])\n",
        "times.setdefault('data', [])\n",
        "mark = False  # Use for starting and stopping the timer\n",
        "max_len = 100\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "def reset(length=100):\n",
        "    global times, mark, max_len\n",
        "    times = {}\n",
        "    times.setdefault('batch', [])\n",
        "    times.setdefault('data', [])\n",
        "    mark = False\n",
        "    max_len = length\n",
        "\n",
        "\n",
        "def start():\n",
        "    global mark, times\n",
        "    mark = True\n",
        "\n",
        "    for k, v in times.items():\n",
        "        if len(v) != 0:\n",
        "            print('Warning, time list is not empty when starting.')\n",
        "\n",
        "\n",
        "def add_batch_time(batch_time):\n",
        "    if mark:\n",
        "        times['batch'].append(batch_time)\n",
        "\n",
        "        inner_time = 0\n",
        "        for k, v in times.items():\n",
        "            if k not in ('batch', 'data'):\n",
        "                inner_time += v[-1]\n",
        "\n",
        "        times['data'].append(batch_time - inner_time)\n",
        "\n",
        "\n",
        "def get_times(time_name):\n",
        "    return_time = []\n",
        "    for name in time_name:\n",
        "        return_time.append(np.mean(times[name]))\n",
        "\n",
        "    return return_time\n",
        "\n",
        "\n",
        "class counter:\n",
        "    def __init__(self, name, trt_mode=False):\n",
        "        self.name = name\n",
        "        self.times = times\n",
        "        self.mark = mark\n",
        "        self.max_len = max_len\n",
        "        self.trt_mode = trt_mode\n",
        "\n",
        "        for v in times.values():\n",
        "            if len(v) >= self.max_len:  # pop the first item if the list is full\n",
        "                v.pop(0)\n",
        "\n",
        "    def __enter__(self):\n",
        "        if self.mark:\n",
        "            if cuda and (not self.trt_mode):  # cuda operation in torch should be forbidden when run by TensorRT\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            self.times.setdefault(self.name, [])\n",
        "            self.times[self.name].append(time.perf_counter())\n",
        "\n",
        "    def __exit__(self, e, ev, t):\n",
        "        if self.mark:\n",
        "            if cuda and (not self.trt_mode):\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            self.times[self.name][-1] = time.perf_counter() - self.times[self.name][-1]\n"
      ],
      "metadata": {
        "id": "pG1fE-Sy6ysd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\" Adapted from torchvision.models.resnet \"\"\"\n",
        "\n",
        "    def __init__(self, layers, block=Bottleneck, norm_layer=nn.BatchNorm2d):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_base_layers = len(layers)\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.channels = []\n",
        "        self.norm_layer = norm_layer\n",
        "        self.inplanes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self._make_layer(block, 64, layers[0])\n",
        "        self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                                 kernel_size=1, stride=stride, bias=False),\n",
        "                                       self.norm_layer(planes * block.expansion))\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample, self.norm_layer)]\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, norm_layer=self.norm_layer))\n",
        "\n",
        "        layer = nn.Sequential(*layers)\n",
        "\n",
        "        self.channels.append(planes * block.expansion)\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Returns a list of convouts for each layer. \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        outs = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            outs.append(x)\n",
        "\n",
        "        return tuple(outs)\n",
        "\n",
        "    def init_backbone(self, path):\n",
        "        \"\"\" Initializes the backbone weights for training. \"\"\"\n",
        "        state_dict = torch.load(path)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "        print(f'\\nBackbone is initiated with {path}.\\n')\n"
      ],
      "metadata": {
        "id": "cqcovV3W683c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import types\n",
        "\n",
        "args= types.SimpleNamespace()\n",
        "\n",
        "args.klas = 'res101_coco'\n",
        "args.train_bs = 8\n",
        "args.img_size = 544\n",
        "args.resume = None\n",
        "args.val_interval = 4000\n",
        "args.img_size = 544\n",
        "args.weight = None\n",
        "args.traditional_nms = False\n",
        "args.val_num = -1\n",
        "args.coco_api = False\n",
        "\n",
        "# cfg = types.SimpleNamespace()"
      ],
      "metadata": {
        "id": "BENVb5upjt6M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import time\n",
        "import torch.utils.data as data\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "\n",
        "iou_thres = [x / 100 for x in range(50, 100, 5)]\n",
        "make_json = MakeJson()\n",
        "\n",
        "\n",
        "def evaluate(net, cfg, step=None):\n",
        "    dataset = COCODetection(cfg, mode='val')\n",
        "    data_loader = data.DataLoader(dataset, 1, num_workers=4, shuffle=False, pin_memory=True, collate_fn=val_collate)\n",
        "    ds = len(data_loader)\n",
        "    progress_bar = ProgressBar(40, ds)\n",
        "    reset()\n",
        "\n",
        "    ap_data = {'box': [[APDataObject() for _ in cfg.class_names] for _ in iou_thres],\n",
        "               'mask': [[APDataObject() for _ in cfg.class_names] for _ in iou_thres]}\n",
        "\n",
        "    for i, (img, gt, gt_masks, img_h, img_w) in enumerate(data_loader):\n",
        "        if i == 1:\n",
        "            start()\n",
        "\n",
        "        if cfg.cuda:\n",
        "            img, gt, gt_masks = img.cuda(), gt.cuda(), gt_masks.cuda()\n",
        "\n",
        "        with torch.no_grad(), counter('forward'):\n",
        "            class_p, box_p, coef_p, proto_p = net(img)\n",
        "\n",
        "        with counter('nms'):\n",
        "            ids_p, class_p, box_p, coef_p, proto_p = nms(class_p, box_p, coef_p, proto_p, net.anchors, cfg)\n",
        "\n",
        "        with counter('after_nms'):\n",
        "            ids_p, class_p, boxes_p, masks_p = after_nms(ids_p, class_p, box_p, coef_p, proto_p, img_h, img_w)\n",
        "            if ids_p is None:\n",
        "                continue\n",
        "\n",
        "        with counter('metric'):\n",
        "            ids_p = list(ids_p.cpu().numpy().astype(int))\n",
        "            class_p = list(class_p.cpu().numpy().astype(float))\n",
        "\n",
        "            if cfg.coco_api:\n",
        "                boxes_p = boxes_p.cpu().numpy()\n",
        "                masks_p = masks_p.cpu().numpy()\n",
        "\n",
        "                for j in range(masks_p.shape[0]):\n",
        "                    if (boxes_p[j, 3] - boxes_p[j, 1]) * (boxes_p[j, 2] - boxes_p[j, 0]) > 0:\n",
        "                        make_json.add_bbox(dataset.ids[i], ids_p[j], boxes_p[j, :], class_p[j])\n",
        "                        make_json.add_mask(dataset.ids[i], ids_p[j], masks_p[j, :, :], class_p[j])\n",
        "            else:\n",
        "                prep_metrics(ap_data, ids_p, class_p, boxes_p, masks_p, gt, gt_masks, img_h, img_w, iou_thres)\n",
        "\n",
        "        aa = time.perf_counter()\n",
        "        if i > 0:\n",
        "            batch_time = aa - temp\n",
        "            add_batch_time(batch_time)\n",
        "        temp = aa\n",
        "\n",
        "        if i > 0:\n",
        "            t_t, t_d, t_f, t_nms, t_an, t_me = get_times(['batch', 'data', 'forward',\n",
        "                                                                'nms', 'after_nms', 'metric'])\n",
        "            fps, t_fps = 1 / (t_d + t_f + t_nms + t_an), 1 / t_t\n",
        "            bar_str = progress_bar.get_bar(i + 1)\n",
        "            print(f'\\rTesting: {bar_str} {i + 1}/{ds}, fps: {fps:.2f} | total fps: {t_fps:.2f} | '\n",
        "                  f't_t: {t_t:.3f} | t_d: {t_d:.3f} | t_f: {t_f:.3f} | t_nms: {t_nms:.3f} | '\n",
        "                  f't_after_nms: {t_an:.3f} | t_metric: {t_me:.3f}', end='')\n",
        "\n",
        "    table, box_row, mask_row = calc_map(ap_data, iou_thres, len(cfg.class_names), step=step)\n",
        "    print(table)\n",
        "    return table, box_row, mask_row\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    # Load or initialize configuration\n",
        "    # prefix = re.findall(r'best_\\d+\\.\\d+_', args.weight)[0] if args.weight else 'best_'\n",
        "    # suffix = re.findall(r'_\\d+\\.pth', args.weight)[0] if args.weight else '.pth'\n",
        "    # args.klas = args.weight.split(prefix)[-1].split(suffix)[0] if args.weight else 'res101_coco'\n",
        "    args.klas = 'res101_coco'\n",
        "    cfg = get_config(args, mode='val')  # Assuming get_config initializes training configuration\n",
        "\n",
        "    net = Yolact(cfg)\n",
        "\n",
        "    if not args.weight:\n",
        "        # net.apply(init_weights)  # Assuming init_weights is a function that initializes the model weights\n",
        "        net.init_weights(net)\n",
        "        # init_weights(net)\n",
        "        # net.init_weights()\n",
        "    else:\n",
        "        net.load_weights(cfg.weight, cfg.cuda)\n",
        "    net.eval()\n",
        "\n",
        "    if cfg.cuda:\n",
        "        cudnn.benchmark = True\n",
        "        cudnn.fastest = True\n",
        "        net = net.cuda()\n",
        "\n",
        "    evaluate(net, cfg)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_TXmgsO7hVo",
        "outputId": "8910bd10-d6c0-4623-d6c9-fe9659d662b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----No GPU found, validate on CPU.-----\n",
            "\n",
            "------------------------------res101_coco------------------------------\n",
            "mode: val\n",
            "cuda: False\n",
            "gpu_id: 0\n",
            "img_size: 544\n",
            "class_names: ('cells', 'kernel')\n",
            "num_classes: 3\n",
            "scales: [24, 48, 96, 192, 384]\n",
            "aspect_ratios: [1, 0.5, 2]\n",
            "weight: None\n",
            "val_imgs: /content/drive/MyDrive/custom_dataset/valid/\n",
            "val_ann: /content/drive/MyDrive/custom_dataset/valid/_annotations.coco.json\n",
            "val_bs: 1\n",
            "val_num: -1\n",
            "coco_api: False\n",
            "traditional_nms: False\n",
            "nms_score_thre: 0.05\n",
            "nms_iou_thre: 0.5\n",
            "top_k: 200\n",
            "max_detections: 200\n",
            "\n",
            "Model initialized.\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=1.99s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: ████████████████████████████████████████ 123/123, fps: 0.24 | total fps: 0.17 | t_t: 5.846 | t_d: 0.050 | t_f: 3.201 | t_nms: 0.029 | t_after_nms: 0.888 | t_metric: 1.683\n",
            "Calculating mAP...\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|      | all | 50  | 55  | 60  | 65  | 70  | 75  | 80  | 85  | 90  | 95  |\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| box  | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
            "| mask | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
            "+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorboardX\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from tensorboardX import SummaryWriter\n",
        "# import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "cfg = get_config(args, mode='train')\n",
        "cfg_name = cfg.__class__.__name__\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# if torch.cuda.is_available():\n",
        "#     num_gpu = torch.cuda.device_count()\n",
        "#     print('num of GPU: {num_gpu}')\n",
        "#     main_gpu = 0\n",
        "#     torch.cuda.set_device(main_gpu)\n",
        "#     device = torch.device(\"cuda\", main_gpu)\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "\n",
        "net = Yolact(cfg)\n",
        "net.to(device)\n",
        "\n",
        "# net.train()\n",
        "\n",
        "# if not args.weight:\n",
        "#         net.apply(init_weights)\n",
        "\n",
        "# if args.resume:\n",
        "#     assert re.findall(r'res.+_[a-z]+', args.resume)[0] == cfg_name, 'Resume weight is not compatible with current cfg.'\n",
        "#     net.load_weights(cfg.weight, cfg.cuda)\n",
        "#     start_step = int(cfg.weight.split('.pth')[0].split('_')[-1])\n",
        "# else:\n",
        "#     net.backbone.init_backbone(cfg.weight)\n",
        "#     start_step = 0\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=cfg.lr, momentum=0.9, weight_decay=5e-4)\n",
        "num_epochs = 600\n",
        "\n",
        "dataset = COCODetection(cfg, mode='train')\n",
        "\n",
        "# train_sampler = None\n",
        "if cfg.cuda:\n",
        "    cudnn.benchmark = True\n",
        "    cudnn.fastest = True\n",
        "\n",
        "# shuffle must be False if sampler is specified\n",
        "data_loader = DataLoader(dataset, args.train_bs, shuffle=True, pin_memory=False)\n",
        "# data_loader = data.DataLoader(dataset, cfg.bs_per_gpu, num_workers=0, shuffle=False,\n",
        "#                               collate_fn=train_collate, pin_memory=True) sampler=train_sampler)\n",
        "start_step = 0\n",
        "epoch_seed = 0\n",
        "map_tables = []\n",
        "# training = True\n",
        "reset()\n",
        "step = start_step\n",
        "val_step = start_step\n",
        "writer = SummaryWriter(f'/content/drive/MyDrive/yolact/tensorboard_log/{cfg_name}')\n",
        "\n",
        "\n",
        "try:  # try-except can shut down all processes after Ctrl + C.\n",
        "    # while training:\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        train_loss = 0.0\n",
        "        # if train_sampler:\n",
        "        #     epoch_seed += 1\n",
        "        #     train_sampler.set_epoch(epoch_seed)\n",
        "\n",
        "        for images, targets, masks in data_loader:\n",
        "            if not cfg.cuda:\n",
        "              images = images.to(device)\n",
        "              targets = targets.to(device)\n",
        "              masks = masks.to(device)\n",
        "            if cfg.warmup_until > 0 and step <= cfg.warmup_until:  # warm up learning rate.\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = (cfg.lr - cfg.warmup_init) * (step / cfg.warmup_until) + cfg.warmup_init\n",
        "\n",
        "            if step in cfg.lr_steps:  # learning rate decay.\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = cfg.lr * 0.1 ** cfg.lr_steps.index(step)\n",
        "\n",
        "            if cfg.cuda:\n",
        "                images = images.cuda().detach()\n",
        "                targets = [ann.cuda().detach() for ann in targets]\n",
        "                masks = [mask.cuda().detach() for mask in masks]\n",
        "\n",
        "            with counter('for+loss'):\n",
        "                loss_c, loss_b, loss_m, loss_s = net(images, targets, masks)\n",
        "\n",
        "                if cfg.cuda:\n",
        "                    all_losses = [loss_c, loss_b, loss_m, loss_s]\n",
        "                    all_loss = sum(all_losses)\n",
        "\n",
        "\n",
        "            with counter('backward'):\n",
        "                loss_total = loss_c + loss_b + loss_m + loss_s\n",
        "                optimizer.zero_grad()\n",
        "                loss_total.backward()\n",
        "\n",
        "            with counter('update'):\n",
        "                optimizer.step()\n",
        "\n",
        "            time_this = time.time()\n",
        "            if step > start_step:\n",
        "                batch_time = time_this - time_last\n",
        "                add_batch_time(batch_time)\n",
        "            time_last = time_this\n",
        "\n",
        "            if step % 10 == 0 and step != start_step:\n",
        "                if (not cfg.cuda):\n",
        "                    cur_lr = optimizer.param_groups[0]['lr']\n",
        "                    time_name = ['batch', 'data', 'for+loss', 'backward', 'update']\n",
        "                    t_t, t_d, t_fl, t_b, t_u = get_times(time_name)\n",
        "                    seconds = (cfg.lr_steps[-1] - step) * t_t\n",
        "                    eta = str(datetime.timedelta(seconds=seconds)).split('.')[0]\n",
        "\n",
        "                    # Get the mean loss across all GPUS for printing, seems need to call .item(), not sure\n",
        "                    # l_c = all_loss[0].item() / num_gpu if main_gpu else loss_c.item()\n",
        "                    # l_b = all_loss[1].item() / num_gpu if main_gpu else loss_b.item()\n",
        "                    # l_m = all_loss[2].item() / num_gpu if main_gpu else loss_m.item()\n",
        "                    # l_s = all_loss[3].item() / num_gpu if main_gpu else loss_s.item()\n",
        "                    l_c = loss_c.item()\n",
        "                    l_b = loss_b.item()\n",
        "                    l_m = loss_m.item()\n",
        "                    l_s = loss_s.item()\n",
        "\n",
        "                    writer.add_scalar('loss/class', l_c, global_step=step)\n",
        "                    writer.add_scalar('loss/box', l_b, global_step=step)\n",
        "                    writer.add_scalar('loss/mask', l_m, global_step=step)\n",
        "                    writer.add_scalar('loss/semantic', l_s, global_step=step)\n",
        "                    writer.add_scalar('loss/total', loss_total, global_step=step)\n",
        "\n",
        "                    print(f'step: {step} | lr: {cur_lr:.2e} | l_class: {l_c:.3f} | l_box: {l_b:.3f} | '\n",
        "                          f'l_mask: {l_m:.3f} | l_semantic: {l_s:.3f} | t_t: {t_t:.3f} | t_d: {t_d:.3f} | '\n",
        "                          f't_fl: {t_fl:.3f} | t_b: {t_b:.3f} | t_u: {t_u:.3f} | ETA: {eta}')\n",
        "\n",
        "            if args.val_interval > 0 and step % args.val_interval == 0 and step != start_step:\n",
        "                if (not cfg.cuda):\n",
        "                    val_step = step\n",
        "                    net.eval()\n",
        "                    table, box_row, mask_row = evaluate(net.module, cfg, step)\n",
        "                    map_tables.append(table)\n",
        "                    net.train()\n",
        "                    reset()  # training timer and val timer share the same Obj, so reset it to avoid conflict\n",
        "\n",
        "                    writer.add_scalar('mAP/box_map', box_row[1], global_step=step)\n",
        "                    writer.add_scalar('mAP/mask_map', mask_row[1], global_step=step)\n",
        "\n",
        "                    save_best(net.module if cfg.cuda else net, mask_row[1], cfg_name, step)\n",
        "\n",
        "            if (not cfg.cuda) and step == val_step + 1:\n",
        "                start()  # the first iteration after validation should not be included\n",
        "\n",
        "            step += 1\n",
        "            if step >= cfg.lr_steps[-1]:\n",
        "                training = False\n",
        "\n",
        "                if not cfg.cuda:\n",
        "                    save_latest(net.module if cfg.cuda else net, cfg_name, step)\n",
        "\n",
        "                    print('\\nValidation results during training:\\n')\n",
        "                    for table in map_tables:\n",
        "                        print(table, '\\n')\n",
        "\n",
        "                    print(f'Training completed.')\n",
        "\n",
        "                break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    if (not cfg.cuda):\n",
        "        save_latest(net.module if cfg.cuda else net, cfg_name, step)\n",
        "\n",
        "        print('\\nValidation results during training:\\n')\n",
        "        for table in map_tables:\n",
        "            print(table, '\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NTlBv7K47So6",
        "outputId": "deddacb1-0de9-4941-f500-2e64294a75d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----No GPU found, training on CPU.-----\n",
            "\n",
            "------------------------------res101_coco------------------------------\n",
            "mode: train\n",
            "cuda: False\n",
            "gpu_id: 0\n",
            "img_size: 544\n",
            "class_names: ('cells', 'kernel')\n",
            "num_classes: 3\n",
            "scales: [24, 48, 96, 192, 384]\n",
            "aspect_ratios: [1, 0.5, 2]\n",
            "weight: /content/drive/MyDrive/yolact/weights/backbone_res101.pth\n",
            "train_imgs: /content/drive/MyDrive/custom_dataset/train/\n",
            "train_ann: /content/drive/MyDrive/custom_dataset/train/_annotations.coco.json\n",
            "train_bs: 8\n",
            "bs_per_gpu: 8\n",
            "val_interval: 4000\n",
            "bs_factor: 1.0\n",
            "lr: 0.001\n",
            "warmup_init: 0.0001\n",
            "warmup_until: 500\n",
            "lr_steps: (0, 280000, 560000, 620000, 680000)\n",
            "pos_iou_thre: 0.5\n",
            "neg_iou_thre: 0.4\n",
            "conf_alpha: 1\n",
            "bbox_alpha: 1.5\n",
            "mask_alpha: 6.125\n",
            "semantic_alpha: 1\n",
            "masks_to_train: 200\n",
            "val_imgs: /content/drive/MyDrive/custom_dataset/valid/\n",
            "val_ann: /content/drive/MyDrive/custom_dataset/valid/_annotations.coco.json\n",
            "val_bs: 1\n",
            "val_num: -1\n",
            "coco_api: False\n",
            "traditional_nms: False\n",
            "nms_score_thre: 0.05\n",
            "nms_iou_thre: 0.5\n",
            "top_k: 200\n",
            "max_detections: 200\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=7.54s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-50f6aa71f90f>:116: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  img_cropped = img[int(new_y1[0]): int(new_y2[0]), int(new_x1[0]): int(new_x2[0]), :]\n",
            "<ipython-input-4-50f6aa71f90f>:117: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  masks_remained = masks_remained[:, int(new_y1[0]): int(new_y2[0]), int(new_x1[0]): int(new_x2[0])]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-43dd5554dcda>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#     train_sampler.set_epoch(epoch_seed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m               \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-85709248adbe>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-50f6aa71f90f>\u001b[0m in \u001b[0;36mtrain_aug\u001b[0;34m(img, masks, boxes, labels, train_size)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_to_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduring_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_scale_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduring_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multiple of 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_train_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    }
  ]
}